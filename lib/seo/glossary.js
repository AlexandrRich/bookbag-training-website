export const glossaryTerms = {
  "ai-production-gate": {
    term: "AI Production Gate",
    definition: "A checkpoint between AI content generation and customer delivery that routes every AI output through human-authority review lanes — safe to deploy, needs fix, or blocked — before it reaches recipients.",
    fullDescription: "An AI Production Gate is an operational control layer that sits between the moment an AI generates customer-facing content and the moment that content is delivered. Unlike prompt engineering or automated filters, a production gate uses human reviewers with defined authority levels (annotators, QA, SMEs) to make verdict decisions on every message. The gate enforces configurable rubrics, produces audit trails, and generates training data from corrections. Bookbag Intelligence pioneered the AI Production Gate model specifically for outbound messaging — email, SMS, LinkedIn, and scripted communications.",
    whyItMatters: "AI-generated outbound messages can contain hallucinations, compliance violations, brand-damaging tone, or inaccurate claims. Without a production gate, these issues only surface after delivery — when the damage is done. A production gate provides pre-send quality control with documented proof of human oversight.",
    howBookbagHelps: "Bookbag is the AI Production Gate for outbound teams. Every AI message is routed through safe/fix/block verdicts with human authority, audit trails, and training data export. The gate scales from hundreds to millions of messages per month.",
    metaTitle: "What Is an AI Production Gate? | Bookbag Intelligence",
    metaDescription: "An AI Production Gate is a checkpoint between AI generation and delivery that routes every output through human review lanes. Learn how it works and why outbound teams need one.",
    faqs: [
      { question: "How is an AI Production Gate different from prompt engineering?", answer: "Prompt engineering tries to prevent bad output at generation time. A production gate catches what gets through — it's a safety net with human authority, not a filter. Both are valuable, but only a gate provides documented proof of oversight." },
      { question: "Does a production gate slow down delivery?", answer: "Safe messages are auto-approved with no delay. Only flagged items (needs_fix, blocked) enter the human review queue. Most teams see 70-85% auto-approval rates after calibration." },
      { question: "What types of content can a production gate review?", answer: "Any text-based AI output: outbound emails, SMS messages, LinkedIn messages, call scripts, chat responses, and marketing copy." },
    ],
    relatedTerms: ["safe-to-deploy", "needs-fix", "blocked-verdict", "message-gating"],
    relatedPersonas: ["ai-sdr-vendors", "vp-product"],
    relatedComparison: "ai-production-gate-vs-prompt-guardrails",
  },

  "message-gating": {
    term: "Message Gating",
    definition: "The process of evaluating and routing AI-generated messages through defined review lanes before they are sent to recipients.",
    fullDescription: "Message gating is the operational practice of intercepting AI-generated communications before delivery and routing them through a structured evaluation process. Each message receives a verdict — safe to deploy, needs fix, or blocked — based on configured rubrics. Gating ensures that every outbound message meets quality, compliance, and brand standards before reaching the recipient. The concept borrows from manufacturing quality gates, where products pass through checkpoints before shipping.",
    whyItMatters: "Without message gating, AI-generated outbound is effectively uncontrolled. Every message is a gamble — most are fine, but the ones that aren't can damage deliverability, trigger compliance violations, or erode brand trust. Gating provides systematic quality control at scale.",
    howBookbagHelps: "Bookbag implements message gating through a three-lane routing system: safe_to_deploy (auto-approved), needs_fix (QA review), and blocked (SME authority). Every verdict includes rubric citations, reviewer identity, and timestamps for audit compliance.",
    metaTitle: "What Is Message Gating? | Bookbag Intelligence",
    metaDescription: "Message gating evaluates and routes AI-generated messages through review lanes before delivery. Learn how gating prevents quality issues in AI outbound.",
    faqs: [
      { question: "What happens to a 'gated' message?", answer: "It enters the evaluation queue, receives a verdict (safe/fix/block) based on rubric criteria, and either auto-approves, gets corrected by QA, or routes to an SME for final authority." },
      { question: "How is gating different from spam filtering?", answer: "Spam filters evaluate incoming messages. Gating evaluates your outgoing AI messages before they're sent. It's proactive quality control, not reactive filtering." },
      { question: "Can gating handle high volume?", answer: "Yes. The safe lane auto-approves clean messages instantly. Only flagged items need human review. This makes gating scalable even at millions of messages per month." },
    ],
    relatedTerms: ["ai-production-gate", "safe-to-deploy", "needs-fix"],
    relatedPersonas: ["revops", "ai-sdr-vendors"],
    relatedComparison: "bookbag-vs-prompt-engineering",
  },

  "safe-to-deploy": {
    term: "Safe to Deploy",
    definition: "A verdict in the AI Production Gate indicating that an AI-generated message meets all quality, compliance, and brand standards and is approved for delivery without human review.",
    fullDescription: "Safe to deploy is the first of three verdict lanes in the Bookbag production gate. When a message receives this verdict, it means the message passed all configured rubric checks and can be sent to the recipient without additional human review. Safe messages are logged with their verdict for audit purposes but don't enter the review queue. The goal of calibration and rubric refinement is to maximize the safe_to_deploy rate while maintaining quality standards — higher auto-approval rates mean faster throughput and lower review costs.",
    whyItMatters: "High safe_to_deploy rates are the key to scaling AI outbound efficiently. If every message needs human review, the gate becomes a bottleneck. A well-calibrated system auto-approves 70-85% of messages while catching the ones that actually need attention.",
    howBookbagHelps: "Bookbag's safe_to_deploy lane auto-approves messages that meet your rubric criteria. Each safe message is still logged (who generated it, which rubric applied, the verdict) for audit compliance. As your AI improves through training data export, safe rates increase over time.",
    metaTitle: "What Is Safe to Deploy? | Bookbag Intelligence",
    metaDescription: "Safe to deploy is a production gate verdict meaning an AI message meets all standards and is approved for delivery. Learn how it works in AI outbound quality workflows.",
    faqs: [
      { question: "Does safe_to_deploy mean no human ever sees the message?", answer: "Correct — the message auto-approves based on rubric evaluation. However, it's still logged for audit purposes, and QA can sample safe messages for calibration checks." },
      { question: "What percentage of messages are typically safe?", answer: "After initial calibration, most teams see 70-85% safe_to_deploy rates. This varies by industry — regulated sectors may have lower initial rates due to stricter rubrics." },
      { question: "Can safe messages be audited later?", answer: "Yes. Every safe message is logged with its verdict, rubric version, and timestamp. You can review safe messages at any time for calibration or audit purposes." },
    ],
    relatedTerms: ["needs-fix", "blocked-verdict", "ai-production-gate"],
    relatedPersonas: ["revops", "compliance-officer"],
    relatedComparison: "bookbag-vs-manual-review",
  },

  "needs-fix": {
    term: "Needs Fix",
    definition: "A verdict in the AI Production Gate indicating that an AI-generated message has quality or compliance issues that a QA reviewer can correct before the message is approved for delivery.",
    fullDescription: "Needs fix is the second verdict lane in the Bookbag production gate. Messages that aren't clearly safe but aren't severely problematic enter the QA review queue. A QA reviewer examines the flagged issues, makes corrections (rewrites, tone adjustments, fact corrections), and approves the fixed version. The original message, the corrections, and the reviewer's rationale are all logged. Critically, every needs_fix correction becomes a training data pair — the original (bad) and the correction (good) — which can be exported for model improvement.",
    whyItMatters: "The needs_fix lane is where most of the value is created. These corrections build your approved messaging library, generate training data for AI improvement, and prevent quality issues from reaching recipients. Over time, patterns in needs_fix items reveal systematic prompt or model weaknesses.",
    howBookbagHelps: "Bookbag routes needs_fix messages to QA reviewers with flagged issues, rubric citations, and the original message context. Reviewers make corrections in the annotation interface. Every correction is exportable as SFT or DPO training data.",
    metaTitle: "What Is a Needs Fix Verdict? | Bookbag Intelligence",
    metaDescription: "Needs fix is a production gate verdict meaning an AI message has fixable issues. Learn how QA review corrects AI outbound and generates training data.",
    faqs: [
      { question: "What types of issues trigger needs_fix?", answer: "Tone mismatches, minor factual inaccuracies, missing context, weak personalization, borderline compliance language, and other issues that a QA reviewer can correct without SME authority." },
      { question: "How long does the fix process take?", answer: "QA reviewers typically process needs_fix items in minutes. Turnaround depends on queue volume and SLA tier. The correction plus the original message are both logged for audit." },
      { question: "Can needs_fix corrections improve the AI?", answer: "Yes. Every correction is a training pair: the original (rejected) and the corrected version (approved). Export these as SFT, DPO, or ranking data to retrain your models." },
    ],
    relatedTerms: ["safe-to-deploy", "blocked-verdict", "qa-review-workflow"],
    relatedPersonas: ["head-of-cs", "sales-enablement"],
    relatedComparison: "rewrite-workflow-vs-prompt-tweaks",
  },

  "blocked-verdict": {
    term: "Blocked Verdict",
    definition: "A verdict in the AI Production Gate indicating that an AI-generated message has serious issues requiring SME (subject matter expert) review, rationale, and evidence before any decision is made.",
    fullDescription: "Blocked is the most severe verdict in the Bookbag production gate. Messages receiving this verdict have issues too serious for standard QA correction — prohibited claims, compliance violations, potential legal exposure, or severe hallucinations. Blocked messages route to designated SMEs who have final authority. SMEs provide a verdict (approve with corrections, reject entirely) along with documented rationale, evidence citations, and rubric references. Every blocked item creates a high-value audit record showing that the organization exercised appropriate oversight.",
    whyItMatters: "Blocked items represent the highest-risk AI outputs — the ones that could trigger regulatory action, damage brand trust, or create legal liability. The blocked lane ensures these items never ship without expert human authority. For regulated industries, this lane is often the most important compliance control.",
    howBookbagHelps: "Bookbag routes blocked messages to designated SMEs with full context: the original message, flagged issues, rubric citations, and evidence. SMEs provide documented decisions. The entire chain — flag, review, decision, rationale — is immutable and exportable for audit.",
    metaTitle: "What Is a Blocked Verdict? | Bookbag Intelligence",
    metaDescription: "A blocked verdict routes high-risk AI messages to SME experts for review with evidence and rationale. Learn how blocked verdicts protect against compliance violations.",
    faqs: [
      { question: "What triggers a blocked verdict?", answer: "Prohibited claims, compliance violations, severe hallucinations, potential legal exposure, and any issue that exceeds QA reviewer authority. The specific triggers are defined in your rubric configuration." },
      { question: "Who reviews blocked messages?", answer: "Designated SMEs (subject matter experts) with final authority. In regulated industries, this might be compliance officers, legal reviewers, or clinical experts." },
      { question: "How much does a blocked verdict cost?", answer: "Blocked items consume 3 credits (vs. 1 credit for safe or needs_fix) because they require SME-level review with documented rationale and evidence." },
    ],
    relatedTerms: ["sme-escalation", "ai-audit-trail", "needs-fix"],
    relatedPersonas: ["compliance-officer", "finserv"],
    relatedComparison: "human-review-vs-automated-qa",
  },

  "sme-escalation": {
    term: "SME Escalation",
    definition: "The process of routing high-risk AI-generated content to designated subject matter experts who have final authority to approve, correct, or reject the content with documented rationale.",
    fullDescription: "SME escalation is the highest level of review in the AI Production Gate. When a message is classified as blocked, it routes to a subject matter expert — a compliance officer, clinical expert, legal reviewer, or domain authority — who makes the final decision. The SME reviews the original message, the flagged issues, and the rubric criteria, then provides a verdict along with documented rationale and evidence. This creates the strongest possible audit record: a traceable decision by a qualified authority with explicit reasoning.",
    whyItMatters: "Enterprise buyers, regulators, and procurement teams need to see that a qualified human made the final call on high-risk content. SME escalation provides this proof. It's the difference between 'our AI handles it' (risky) and 'a qualified expert reviewed and approved it with evidence' (defensible).",
    howBookbagHelps: "Bookbag provides a dedicated SME review interface with full context, rubric citations, and structured rationale fields. Every SME decision is timestamped, attributed, and immutable. Organizations can designate different SMEs for different project types or compliance domains.",
    metaTitle: "What Is SME Escalation in AI Quality? | Bookbag Intelligence",
    metaDescription: "SME escalation routes high-risk AI content to subject matter experts for final authority decisions. Learn how escalation provides audit-ready compliance proof.",
    faqs: [
      { question: "Who qualifies as an SME?", answer: "That depends on your domain. In financial services, it might be a compliance officer. In healthcare, a clinical reviewer. In B2B SaaS, a senior product marketer. You designate SMEs based on your authority requirements." },
      { question: "How does SME review differ from QA review?", answer: "QA reviewers handle needs_fix items — correctable issues within their authority. SMEs handle blocked items — high-risk issues requiring domain expertise, final authority, and documented rationale." },
      { question: "Is SME rationale part of the audit trail?", answer: "Yes. Every SME decision includes the verdict, rationale text, evidence citations, the rubric version applied, and the SME's identity. All immutable and exportable." },
    ],
    relatedTerms: ["blocked-verdict", "ai-audit-trail", "rubric-versioning"],
    relatedPersonas: ["compliance-officer", "finserv"],
    relatedComparison: "ai-outbound-compliance-vs-legal-review",
  },

  "finra-ai-compliance": {
    term: "FINRA AI Compliance",
    definition: "The application of FINRA advertising and communications rules (particularly Rule 2210) to AI-generated financial services outbound, ensuring AI content meets the same regulatory standards as human-authored communications.",
    fullDescription: "FINRA Rule 2210 governs communications with the public by broker-dealers and registered representatives. As financial services firms adopt AI for outbound messaging, these rules apply equally to AI-generated content. Common AI violations include performance guarantees, promissory language, omitted risk disclosures, and unsubstantiated claims. FINRA has signaled that firms are responsible for supervising AI-generated content with the same rigor as human-authored communications. This creates a need for systematic review processes with documented oversight.",
    whyItMatters: "FINRA standard review fees are $300 per filing ($600 expedited). FTC penalties can exceed $53,000 per violation. A single AI-generated email with a prohibited performance claim can trigger regulatory action. Systematic AI content review is becoming a compliance requirement, not an option.",
    howBookbagHelps: "Bookbag provides FINRA-aware rubric templates that flag common violations: performance guarantees, promissory language, missing disclosures, and unsubstantiated claims. Blocked items route to compliance SMEs. Every decision is audit-ready with timestamped provenance.",
    metaTitle: "FINRA AI Compliance for Outbound Messaging | Bookbag Intelligence",
    metaDescription: "Learn how FINRA rules apply to AI-generated financial services outbound. Catch performance claims, missing disclosures, and prohibited language before delivery.",
    faqs: [
      { question: "Does FINRA specifically address AI-generated content?", answer: "FINRA's communications rules apply to all firm communications regardless of how they're created. Firms must supervise AI-generated content with the same processes used for human-authored communications." },
      { question: "What are the most common AI violations of FINRA 2210?", answer: "Performance guarantees, promissory language, omitted risk disclosures, testimonials without disclaimers, and unsubstantiated claims about investment products or returns." },
      { question: "Can Bookbag replace our FINRA compliance review process?", answer: "No. Bookbag supports and systematizes your compliance review by providing rubric enforcement, SME escalation, and audit trails. Your compliance team retains authority and oversight." },
    ],
    relatedTerms: ["ai-outbound-compliance", "ai-audit-trail", "sme-escalation"],
    relatedPersonas: ["finserv", "compliance-officer"],
    relatedComparison: "ai-outbound-compliance-vs-legal-review",
  },

  "ai-outbound-compliance": {
    term: "AI Outbound Compliance",
    definition: "The practice of ensuring AI-generated outbound communications meet applicable legal, regulatory, and industry standards before delivery to recipients.",
    fullDescription: "AI outbound compliance encompasses all regulatory and legal requirements that apply to AI-generated customer communications. This includes federal regulations (CAN-SPAM, TCPA, FTC Act), industry-specific rules (FINRA, HIPAA, FDCPA), and state-level advertising laws. As AI generates outbound at increasing scale and speed, traditional manual review processes can't keep pace. AI outbound compliance requires systematic, scalable review processes that document oversight and produce audit trails.",
    whyItMatters: "The volume and speed of AI-generated outbound makes compliance more challenging and more important simultaneously. A human writes one email at a time. AI can generate thousands. Each one carries compliance risk. Without systematic review, compliance teams face an impossible task.",
    howBookbagHelps: "Bookbag provides the systematic review infrastructure for AI outbound compliance: configurable rubrics, three-lane routing, SME escalation, and immutable audit trails. Compliance teams define the rules; Bookbag enforces them at scale.",
    metaTitle: "AI Outbound Compliance: Rules & Requirements | Bookbag Intelligence",
    metaDescription: "AI outbound compliance ensures AI-generated messages meet legal and regulatory standards. Learn the requirements and how to implement systematic compliance review.",
    faqs: [
      { question: "What regulations apply to AI outbound?", answer: "Federal: CAN-SPAM, TCPA, FTC Act. Industry: FINRA 2210 (financial), HIPAA (healthcare), FDCPA (collections). State: Various advertising and consumer protection laws. The specific requirements depend on your industry and channels." },
      { question: "Is AI-generated content treated differently by regulators?", answer: "No. Regulators hold firms to the same standards regardless of whether content was human-authored or AI-generated. The key requirement is documented supervision of all customer communications." },
      { question: "How does Bookbag help with compliance documentation?", answer: "Every message, verdict, correction, and SME decision is logged with timestamps, reviewer identity, rubric version, and rationale. This creates the supervision documentation regulators expect." },
    ],
    relatedTerms: ["finra-ai-compliance", "can-spam-ai-messaging", "tcpa-ai-compliance"],
    relatedPersonas: ["compliance-officer", "finserv"],
    relatedComparison: "ai-outbound-compliance-vs-legal-review",
  },

  "can-spam-ai-messaging": {
    term: "CAN-SPAM for AI Messaging",
    definition: "The application of the CAN-SPAM Act requirements to AI-generated commercial email, including truthful headers, honest subject lines, physical address inclusion, and opt-out mechanisms.",
    fullDescription: "The CAN-SPAM Act establishes rules for commercial email communications. AI-generated emails must comply with the same requirements as human-authored ones: truthful header information, non-deceptive subject lines, identification as advertising (when applicable), a valid physical address, and a clear opt-out mechanism. AI presents unique compliance challenges because it can generate subject lines that inadvertently mislead, omit required elements, or create content that doesn't accurately represent the sender.",
    whyItMatters: "CAN-SPAM violations carry penalties up to $51,744 per email. AI can generate thousands of non-compliant emails before anyone notices. Systematic pre-send review is the most effective way to prevent mass CAN-SPAM violations from AI content.",
    howBookbagHelps: "Bookbag rubrics can be configured to check for CAN-SPAM requirements: honest subject lines, sender identification, physical address presence, opt-out mechanism, and content accuracy. Missing elements are flagged as needs_fix or blocked.",
    metaTitle: "CAN-SPAM Compliance for AI Email | Bookbag Intelligence",
    metaDescription: "Learn how CAN-SPAM requirements apply to AI-generated emails. Catch missing opt-outs, deceptive subjects, and compliance gaps before sending.",
    faqs: [
      { question: "Can AI accidentally violate CAN-SPAM?", answer: "Yes. AI can generate misleading subject lines, omit required sender identification, or create content that doesn't include a valid physical address or opt-out mechanism. These are all CAN-SPAM violations." },
      { question: "What's the penalty for CAN-SPAM violations?", answer: "Up to $51,744 per individual email that violates the Act. For AI-generated campaigns at scale, this can add up to significant exposure quickly." },
      { question: "How does Bookbag check for CAN-SPAM compliance?", answer: "Configure rubrics that flag: deceptive subject lines, missing sender identification, absent physical address, missing opt-out mechanism, and misleading content claims. Each violation type gets appropriate routing (needs_fix or blocked)." },
    ],
    relatedTerms: ["ai-outbound-compliance", "tcpa-ai-compliance", "outbound-deliverability-risk"],
    relatedPersonas: ["cold-email-infrastructure", "compliance-officer"],
    relatedComparison: "quality-gate-vs-deliverability-tooling",
  },

  "tcpa-ai-compliance": {
    term: "TCPA AI Compliance",
    definition: "Ensuring AI-generated text messages, calls, and voicemails comply with the Telephone Consumer Protection Act's consent, timing, and content requirements.",
    fullDescription: "The TCPA regulates telephone communications including calls, texts, and voicemails. AI-generated outbound via these channels must comply with prior express consent requirements, time-of-day restrictions, identification requirements, and opt-out provisions. AI introduces unique risks: it can generate messages without verifying consent status, ignore time zone restrictions, or create content that doesn't meet TCPA disclosure requirements.",
    whyItMatters: "TCPA violations carry statutory damages of $500-$1,500 per message. Class action lawsuits for TCPA violations regularly result in multi-million dollar settlements. AI-generated text campaigns at scale represent significant TCPA exposure.",
    howBookbagHelps: "Bookbag can be configured with TCPA-aware rubrics that flag consent compliance issues, timing concerns, missing identification, and opt-out mechanism requirements in AI-generated texts and call scripts.",
    metaTitle: "TCPA Compliance for AI Messaging | Bookbag Intelligence",
    metaDescription: "Learn how TCPA rules apply to AI-generated texts and calls. Prevent consent violations, timing issues, and missing disclosures with systematic AI message review.",
    faqs: [
      { question: "Does the TCPA apply to AI-generated texts?", answer: "Yes. The TCPA applies to all commercial text messages regardless of how they're created. AI-generated texts must comply with the same consent, timing, and content requirements as manually composed ones." },
      { question: "What's the biggest TCPA risk with AI?", answer: "Volume. AI can generate thousands of texts quickly. If any have consent issues, timing violations, or missing disclosures, the per-message statutory damages add up rapidly." },
      { question: "Can Bookbag prevent TCPA violations?", answer: "Bookbag evaluates AI-generated message content against your configured TCPA rubrics — checking for missing identification, opt-out mechanisms, and compliance language. Consent verification and timing are typically handled by your sending infrastructure." },
    ],
    relatedTerms: ["ai-outbound-compliance", "can-spam-ai-messaging", "ai-audit-trail"],
    relatedPersonas: ["lending", "collections"],
    relatedComparison: "ai-outbound-compliance-vs-legal-review",
  },

  "ai-audit-trail": {
    term: "AI Audit Trail",
    definition: "A complete, immutable record of every decision made about AI-generated content — including who reviewed it, when, which rubric applied, the verdict, rationale, and any corrections.",
    fullDescription: "An AI audit trail documents the entire lifecycle of an AI-generated message through the review process. Each record includes: the original AI output, the configured rubric version, the reviewer identity, the timestamp, the verdict (safe/fix/block), any corrections made, the rationale for the decision, and evidence citations. Audit trails are immutable — they cannot be altered after the fact — and tenant-isolated, meaning one organization's audit data is never accessible to another.",
    whyItMatters: "Regulators, enterprise buyers, and procurement teams increasingly require documented proof that AI-generated communications are supervised. An audit trail provides this proof. Without one, claiming 'we review AI output' is unverifiable. With one, every decision is traceable and attributable.",
    howBookbagHelps: "Bookbag generates audit trails automatically for every message that passes through the gate. Trails include full provenance: message content, rubric version, reviewer identity, verdict, corrections, rationale, and timestamp. Exports are immutable and tenant-isolated.",
    metaTitle: "What Is an AI Audit Trail? | Bookbag Intelligence",
    metaDescription: "An AI audit trail documents every review decision on AI content — who, when, what, and why. Learn why audit trails matter for AI governance and compliance.",
    faqs: [
      { question: "What does an audit trail record include?", answer: "Original AI message, rubric version applied, reviewer identity, timestamp, verdict (safe/fix/block), corrections made, rationale, evidence citations, and approval chain." },
      { question: "Can audit trail records be modified?", answer: "No. Bookbag audit records are immutable. Once a decision is logged, it cannot be altered. This ensures the integrity of the supervision record." },
      { question: "Who typically needs audit trail access?", answer: "Compliance officers, risk teams, legal counsel, regulatory examiners, and enterprise procurement. Bookbag provides role-based access controls for audit data." },
    ],
    relatedTerms: ["sme-escalation", "rubric-versioning", "audit-ready-review"],
    relatedPersonas: ["compliance-officer", "finserv"],
    relatedComparison: "bookbag-vs-manual-review",
  },

  "ai-hallucination-detection": {
    term: "AI Hallucination Detection",
    definition: "The process of identifying factually incorrect, fabricated, or unsubstantiated claims in AI-generated content before it reaches recipients.",
    fullDescription: "AI hallucination detection identifies instances where AI models generate content that sounds plausible but is factually wrong — fabricated statistics, nonexistent product features, incorrect company information, or unsubstantiated claims. In outbound messaging, hallucinations are particularly dangerous because they appear in professional communications that recipients trust. Detection requires comparing AI output against known facts, product specifications, and approved messaging — something automated systems struggle with but human reviewers can catch reliably.",
    whyItMatters: "A hallucinated product claim, fabricated case study reference, or incorrect pricing in an outbound email destroys trust and can create legal liability. In regulated industries, hallucinations can constitute fraud or misrepresentation.",
    howBookbagHelps: "Bookbag reviewers evaluate AI messages against your configured rubrics, which can include approved product facts, pricing, features, and claims. Hallucinations are flagged as needs_fix (minor) or blocked (serious) depending on severity.",
    metaTitle: "AI Hallucination Detection in Outbound | Bookbag Intelligence",
    metaDescription: "Detect AI hallucinations in outbound messages before delivery. Catch fabricated claims, wrong facts, and unsubstantiated statements with human review.",
    faqs: [
      { question: "What are common hallucinations in outbound AI?", answer: "Fabricated product features, incorrect pricing, nonexistent integrations, made-up statistics, wrong company information, and unsubstantiated performance claims." },
      { question: "Can automated tools catch hallucinations?", answer: "Some can catch obvious factual errors, but nuanced hallucinations (plausible-sounding but wrong claims) require human domain expertise to identify reliably." },
      { question: "How does Bookbag handle hallucinations?", answer: "Reviewers compare AI output against your rubrics (approved facts, features, pricing). Hallucinations are flagged and corrected. Patterns in hallucination types help identify prompt or model weaknesses." },
    ],
    relatedTerms: ["ai-message-quality", "ai-brand-safety", "gold-standard-rewrites"],
    relatedPersonas: ["b2b-saas", "vp-product"],
    relatedComparison: "bookbag-vs-prompt-engineering",
  },

  "ai-message-quality": {
    term: "AI Message Quality",
    definition: "The overall standard of AI-generated outbound communications measured across dimensions including accuracy, tone, compliance, personalization relevance, and conversion effectiveness.",
    fullDescription: "AI message quality is a composite measure of how well AI-generated outbound meets multiple standards simultaneously. A high-quality AI message is factually accurate, appropriately toned for the recipient, compliant with applicable regulations, relevantly personalized, and effective at driving the intended action. Quality measurement requires structured evaluation against defined rubrics, not subjective opinion. Systematic measurement reveals quality trends, identifies model weaknesses, and enables continuous improvement.",
    whyItMatters: "Message quality directly impacts deliverability, conversion rates, brand perception, and compliance risk. Low-quality AI output damages all four simultaneously. Systematic quality measurement is the foundation for AI improvement.",
    howBookbagHelps: "Bookbag evaluates every AI message against configurable quality rubrics covering accuracy, tone, compliance, and effectiveness. Quality metrics (pass/fix/block rates, failure categories, trends) provide executive-level visibility into AI performance.",
    metaTitle: "AI Message Quality: Measurement & Improvement | Bookbag Intelligence",
    metaDescription: "AI message quality measures accuracy, tone, compliance, and effectiveness. Learn how to systematically measure and improve AI outbound quality.",
    faqs: [
      { question: "How do you measure AI message quality?", answer: "Through structured rubric evaluation across defined dimensions: factual accuracy, tone appropriateness, compliance adherence, personalization relevance, and conversion effectiveness. Each dimension has defined criteria." },
      { question: "What's a 'good' quality rate?", answer: "After calibration, most teams target 70-85% safe_to_deploy (auto-approval) rates. This means 70-85% of AI messages meet all quality standards without human intervention." },
      { question: "How does quality improve over time?", answer: "Corrections from needs_fix and blocked items become training data. Export SFT, DPO, and ranking pairs to retrain models. Quality typically improves 10-20% over the first 3-6 months of systematic gating." },
    ],
    relatedTerms: ["ai-production-gate", "gold-standard-rewrites", "qa-review-workflow"],
    relatedPersonas: ["revops", "sales-enablement"],
    relatedComparison: "bookbag-vs-internal-qa",
  },

  "outbound-deliverability-risk": {
    term: "Outbound Deliverability Risk",
    definition: "The risk that AI-generated outbound messages damage sender reputation, trigger spam filters, or reduce inbox placement rates.",
    fullDescription: "Outbound deliverability risk is the threat that AI-generated emails harm your ability to reach inboxes. AI can generate content that triggers spam filters (spammy language patterns, excessive links, formatting red flags), generates recipient complaints (irrelevant personalization, aggressive tone), or violates bulk sender requirements (Gmail/Yahoo rules for authentication, unsubscribe, spam rate thresholds). Deliverability damage is expensive to reverse — domain rehabilitation can take weeks or months.",
    whyItMatters: "Gmail and Yahoo's 2024 bulk sender rules made deliverability a business-critical metric. For AI outbound vendors, deliverability failures cause customer churn. For enterprises, they halt revenue-generating campaigns. Protecting deliverability requires catching risky content before it's sent.",
    howBookbagHelps: "Bookbag rubrics can flag deliverability risk factors: spammy language patterns, excessive link density, missing compliance elements, aggressive tone, and formatting issues. Catching these pre-send protects sender reputation and inbox placement.",
    metaTitle: "Outbound Deliverability Risk from AI | Bookbag Intelligence",
    metaDescription: "AI-generated outbound can damage deliverability. Learn how to identify and prevent AI content that triggers spam filters, complaints, and inbox placement drops.",
    faqs: [
      { question: "How does AI content hurt deliverability?", answer: "Through spam-trigger language patterns, excessive links, aggressive tone that generates complaints, missing compliance elements (unsubscribe, identification), and formatting that spam filters flag." },
      { question: "Can Bookbag check deliverability risk?", answer: "Yes. Configure rubrics to flag known deliverability risk factors. Messages with high risk scores route to needs_fix for correction before sending." },
      { question: "How long does it take to recover from deliverability damage?", answer: "Domain rehabilitation can take 2-8 weeks depending on severity. Prevention through pre-send review is dramatically cheaper than recovery." },
    ],
    relatedTerms: ["can-spam-ai-messaging", "ai-brand-safety", "message-gating"],
    relatedPersonas: ["cold-email-infrastructure", "deliverability-agencies"],
    relatedComparison: "quality-gate-vs-deliverability-tooling",
  },

  "ai-brand-safety": {
    term: "AI Brand Safety",
    definition: "Protecting brand reputation by ensuring AI-generated communications maintain approved tone, messaging standards, and factual accuracy.",
    fullDescription: "AI brand safety ensures that AI-generated outbound content represents your brand accurately and positively. This includes tone consistency (matching your brand voice), messaging accuracy (only claiming features that exist), positioning alignment (staying on-message), and risk avoidance (no content that could embarrass the brand). Brand safety is particularly challenging with AI because models don't inherently understand your brand guidelines — they generate plausible content that may or may not align with your standards.",
    whyItMatters: "One off-brand AI message can become a screenshot on social media. Consistent brand misrepresentation erodes trust with prospects and customers. AI-generated content represents your brand at scale — quality control is brand protection.",
    howBookbagHelps: "Bookbag enforces brand standards through configurable rubrics: approved terminology, tone guidelines, factual claims, and positioning language. Every AI message is evaluated against these standards before delivery.",
    metaTitle: "AI Brand Safety for Outbound | Bookbag Intelligence",
    metaDescription: "Protect your brand from AI-generated messaging that's off-tone, inaccurate, or embarrassing. Learn how brand safety rubrics prevent AI brand damage.",
    faqs: [
      { question: "What are common AI brand safety issues?", answer: "Wrong tone, hallucinated product features, inaccurate pricing, competitor mentions, off-brand humor, aggressive sales language, and claims that don't match your positioning." },
      { question: "How do you enforce brand standards with AI?", answer: "Define brand rubrics: approved terminology, tone guidelines, factual claims. Every AI message is evaluated against these standards. Off-brand content is flagged and corrected before delivery." },
      { question: "Can brand standards evolve over time?", answer: "Yes. Rubrics are version-controlled. Update your brand standards as they evolve. Version stamps on audit records show which standards applied to each decision." },
    ],
    relatedTerms: ["ai-hallucination-detection", "ai-message-quality", "taxonomy-config"],
    relatedPersonas: ["sales-enablement", "b2b-saas"],
    relatedComparison: "bookbag-vs-prompt-engineering",
  },

  "sft-export": {
    term: "SFT Export",
    definition: "Supervised Fine-Tuning export — extracting human-corrected message pairs (original AI output + approved correction) in formats suitable for fine-tuning language models.",
    fullDescription: "SFT (Supervised Fine-Tuning) export converts human corrections from the review process into training data pairs. Each pair consists of the original AI-generated message (input) and the human-approved correction (target output). These pairs are used to fine-tune language models to produce higher-quality output aligned with your specific standards. SFT is the most straightforward form of training data — it directly teaches the model what 'good' looks like by showing corrected examples.",
    whyItMatters: "Every human correction in the production gate represents a real-world quality signal. SFT export turns these corrections into model improvement — the AI literally learns from its mistakes. Over time, this creates a flywheel: better AI → fewer corrections → higher safe_to_deploy rates → lower review costs.",
    howBookbagHelps: "Bookbag exports approved-only SFT pairs from needs_fix and blocked corrections. Exports are tenant-isolated, immutable, and include provenance data. No AI re-evaluation is run during export — it's pure data transformation of human decisions.",
    metaTitle: "SFT Export for AI Training Data | Bookbag Intelligence",
    metaDescription: "Export supervised fine-tuning data from human corrections. Turn AI message reviews into training pairs that improve your models over time.",
    faqs: [
      { question: "What format are SFT exports?", answer: "Standard training data formats: JSON pairs with input (original) and target (correction). Compatible with common fine-tuning frameworks." },
      { question: "Do SFT exports include all messages?", answer: "Only approved corrections are exported. Safe_to_deploy messages are not included since they don't represent corrections. The data is exclusively from human-reviewed and approved items." },
      { question: "How much training data do I need?", answer: "Even 50-100 high-quality correction pairs can measurably improve model performance for specific tasks. Most teams accumulate meaningful training data within the first month of production gating." },
    ],
    relatedTerms: ["dpo-training-data", "preference-ranking-data", "gold-standard-rewrites"],
    relatedPersonas: ["vp-product", "ai-sdr-vendors"],
    relatedComparison: "rewrite-workflow-vs-prompt-tweaks",
  },

  "dpo-training-data": {
    term: "DPO Training Data",
    definition: "Direct Preference Optimization data — pairs of AI outputs where one version is human-preferred over another, used to align language models with human quality standards.",
    fullDescription: "DPO (Direct Preference Optimization) training data consists of pairs where a human has indicated which version is better. In the context of AI outbound, this typically means the original AI message (rejected) paired with the human-corrected version (preferred). DPO is a powerful alignment technique because it teaches models not just what to produce, but what to prefer — aligning the model's generation tendencies with human quality standards. DPO data from production review is particularly valuable because it reflects real-world quality judgments, not synthetic preferences.",
    whyItMatters: "DPO data teaches AI models to prefer the kinds of outputs that humans approve. This is more nuanced than SFT alone — instead of just showing 'good' examples, you're showing the model the difference between 'what you generated' and 'what the expert preferred.' This directly improves generation quality for future outputs.",
    howBookbagHelps: "Bookbag automatically structures corrections as DPO pairs: the original AI message (rejected) and the approved correction (preferred). Exports include provenance data showing which rubric, reviewer, and verdict generated the preference signal.",
    metaTitle: "DPO Training Data from AI Reviews | Bookbag Intelligence",
    metaDescription: "Generate DPO preference pairs from human AI message reviews. Turn corrections into model alignment data that teaches AI what quality looks like.",
    faqs: [
      { question: "How is DPO different from SFT?", answer: "SFT teaches the model what to produce (input → output pairs). DPO teaches the model what to prefer (rejected vs. preferred pairs). DPO is often more effective for aligning generation quality with human standards." },
      { question: "Where do DPO pairs come from?", answer: "Every needs_fix correction creates a DPO pair: the original message (rejected) and the corrected version (preferred). The human reviewer's correction is the preference signal." },
      { question: "How many DPO pairs do I need?", answer: "Research shows meaningful alignment improvements with as few as 100-500 high-quality preference pairs. Production review generates these organically over the first weeks of gating." },
    ],
    relatedTerms: ["sft-export", "preference-ranking-data", "gold-standard-rewrites"],
    relatedPersonas: ["vp-product", "ai-sdr-vendors"],
    relatedComparison: "rewrite-workflow-vs-prompt-tweaks",
  },

  "preference-ranking-data": {
    term: "Preference Ranking Data",
    definition: "Ordered rankings of multiple AI output variations by human reviewers, used to train models on quality gradients rather than binary good/bad distinctions.",
    fullDescription: "Preference ranking data extends beyond simple A/B preference pairs. Reviewers rank multiple AI output variations from best to worst, creating ordered quality gradients. This data teaches models nuanced quality distinctions — not just 'this is good and that is bad,' but 'this is best, this is acceptable, and this is poor, for these specific reasons.' Ranking data is particularly valuable for calibrating AI outputs across multiple quality dimensions simultaneously.",
    whyItMatters: "Binary good/bad labels lose information. Rankings capture quality gradients that help models understand degrees of quality. A message might be 'acceptable but not great' — ranking data captures this nuance where binary labels can't.",
    howBookbagHelps: "Bookbag supports ranking tasks where reviewers order multiple AI variations. These rankings are exportable as training data with full provenance and rubric context.",
    metaTitle: "Preference Ranking Data for AI Training | Bookbag Intelligence",
    metaDescription: "Generate preference ranking data from human reviews of AI outputs. Capture quality gradients that improve model alignment beyond binary good/bad labels.",
    faqs: [
      { question: "How is ranking different from DPO?", answer: "DPO uses pairs (preferred vs. rejected). Ranking orders multiple outputs from best to worst. Rankings capture finer quality gradients and can involve more than two options." },
      { question: "When should I use ranking vs. DPO?", answer: "Use DPO when you have clear before/after corrections. Use ranking when comparing multiple model outputs, prompt variations, or generation strategies against each other." },
      { question: "How many items should reviewers rank?", answer: "Typically 2-5 variations per ranking task. More than 5 becomes cognitively difficult for reviewers and produces less reliable rankings." },
    ],
    relatedTerms: ["dpo-training-data", "sft-export", "annotator-calibration"],
    relatedPersonas: ["vp-product", "ai-sdr-vendors"],
    relatedComparison: "rewrite-workflow-vs-prompt-tweaks",
  },

  "gold-standard-rewrites": {
    term: "Gold Standard Rewrites",
    definition: "Expert-corrected versions of AI-generated messages that serve as the approved reference examples for quality, tone, compliance, and effectiveness.",
    fullDescription: "Gold standard rewrites are the human-approved, corrected versions of AI outputs that didn't meet quality standards. When a QA reviewer or SME fixes an AI message, the corrected version becomes a 'gold standard' — an authoritative example of what the AI should have produced. Gold rewrites serve multiple purposes: they're delivered to the customer (replacing the flawed original), they become training data for model improvement, and they build an approved messaging library that teams can reference.",
    whyItMatters: "Gold rewrites are the highest-value output of the review process. They solve the immediate problem (fixing a bad message), improve the AI (training data), and build institutional knowledge (approved messaging library). A growing library of gold rewrites is a compounding asset.",
    howBookbagHelps: "Every needs_fix and blocked correction in Bookbag produces a gold rewrite. These are cataloged, searchable, and exportable as training data. The gold rewrite library grows organically as your team reviews more messages.",
    metaTitle: "Gold Standard Rewrites in AI Quality | Bookbag Intelligence",
    metaDescription: "Gold standard rewrites are expert-corrected AI messages that set quality benchmarks. Learn how they improve AI models and build approved messaging libraries.",
    faqs: [
      { question: "Who creates gold standard rewrites?", answer: "QA reviewers and SMEs create gold rewrites as part of the normal review process. When they correct a needs_fix or blocked message, the correction becomes the gold standard." },
      { question: "How are gold rewrites used?", answer: "Three ways: (1) they replace the flawed original for delivery, (2) they become SFT/DPO training data for model improvement, and (3) they build a searchable approved messaging library." },
      { question: "How many gold rewrites should we aim for?", answer: "Quality over quantity. Even 20 gold rewrites in a free audit are valuable. In production, most teams accumulate hundreds within the first month from needs_fix corrections alone." },
    ],
    relatedTerms: ["sft-export", "dpo-training-data", "needs-fix"],
    relatedPersonas: ["sales-enablement", "ai-sdr-vendors"],
    relatedComparison: "rewrite-workflow-vs-prompt-tweaks",
  },

  "human-in-the-loop-ai": {
    term: "Human-in-the-Loop AI",
    definition: "An AI system design where human reviewers participate in the decision-making process, providing oversight, corrections, and authority that the AI alone cannot provide.",
    fullDescription: "Human-in-the-loop (HITL) AI describes systems where humans are integrated into the AI's operational workflow — not as an afterthought, but as an essential component. In AI outbound, HITL means human reviewers evaluate AI-generated messages, make verdict decisions, provide corrections, and exercise authority over high-risk items. The key distinction from 'human review' is that HITL is systematic, structured, and produces data: every human decision feeds back into the system as training data, calibration signals, and audit records.",
    whyItMatters: "Pure automation scales but lacks judgment. Pure human review doesn't scale. Human-in-the-loop combines the scale of AI with the judgment of humans. For outbound messaging, this means scaling to millions of messages while maintaining the quality, compliance, and brand safety that only human authority can provide.",
    howBookbagHelps: "Bookbag is built as a human-in-the-loop system. Humans operate at three authority levels (annotators, QA, SMEs) within structured workflows. Every human decision is captured, produces training data, and contributes to audit trails.",
    metaTitle: "Human-in-the-Loop AI for Outbound | Bookbag Intelligence",
    metaDescription: "Human-in-the-loop AI combines AI scale with human judgment. Learn how HITL workflows improve AI outbound quality and produce training data.",
    faqs: [
      { question: "Doesn't human-in-the-loop slow things down?", answer: "Only for items that need it. Safe messages auto-approve instantly. Humans focus on the 15-30% of messages that actually need review. The result is faster than reviewing everything manually." },
      { question: "How is HITL different from just having humans review?", answer: "Structure. HITL is a designed system: defined authority levels, rubric enforcement, calibration processes, data capture, and feedback loops. 'Human review' can be ad-hoc. HITL is systematic and produces compounding value." },
      { question: "Does HITL replace AI?", answer: "No. HITL makes AI better. Human corrections become training data. Over time, the AI produces more safe_to_deploy output, reducing the human review burden. It's a flywheel, not a replacement." },
    ],
    relatedTerms: ["ai-production-gate", "sme-escalation", "annotator-calibration"],
    relatedPersonas: ["vp-product", "compliance-officer"],
    relatedComparison: "human-review-vs-automated-qa",
  },

  "qa-review-workflow": {
    term: "QA Review Workflow",
    definition: "A structured process where quality assurance reviewers evaluate, correct, and approve AI-generated content using defined rubrics and authority levels.",
    fullDescription: "A QA review workflow is the operational process that handles needs_fix items in the production gate. QA reviewers receive flagged messages with context (the rubric citations, the flagged issues, the original AI output), make corrections, and approve the fixed version. The workflow includes queue management, rubric enforcement, correction tracking, and escalation paths for items that exceed QA authority. Well-designed QA workflows are consistent (rubric-driven), efficient (focused on items that need attention), and valuable (every correction produces training data).",
    whyItMatters: "Without a structured QA workflow, review is inconsistent and undocumented. Different reviewers apply different standards. Corrections aren't captured. Patterns in AI failures go unnoticed. A structured workflow ensures consistency, captures value, and enables continuous improvement.",
    howBookbagHelps: "Bookbag provides the QA workflow infrastructure: queue management, rubric-driven evaluation interface, correction tools, escalation to SME, and automatic training data capture from every review decision.",
    metaTitle: "QA Review Workflow for AI Content | Bookbag Intelligence",
    metaDescription: "A QA review workflow structures how reviewers evaluate and correct AI content. Learn how structured review improves quality and generates training data.",
    faqs: [
      { question: "What does a QA reviewer actually do?", answer: "QA reviewers receive flagged AI messages, evaluate them against rubric criteria, make corrections (rewrites, tone fixes, fact corrections), and approve the corrected version. They also escalate items that exceed their authority to SMEs." },
      { question: "How are QA reviewers calibrated?", answer: "Through gold set evaluation (reviewing pre-labeled examples), rubric training, and ongoing quality sampling. Inter-annotator agreement metrics track consistency across reviewers." },
      { question: "How many QA reviewers do I need?", answer: "Depends on your volume and turnaround SLA. Bookbag manages workforce scaling. Most teams start with 2-5 reviewers for pilot volume and scale based on needs." },
    ],
    relatedTerms: ["needs-fix", "annotator-calibration", "taxonomy-config"],
    relatedPersonas: ["revops", "head-of-cs"],
    relatedComparison: "bookbag-vs-internal-qa",
  },

  "taxonomy-config": {
    term: "Taxonomy Config",
    definition: "A configurable schema that defines the rubrics, label definitions, scoring criteria, and review rules for a specific project or compliance domain.",
    fullDescription: "Taxonomy configuration is the system that turns customer requirements into machine-enforceable rules. Instead of building custom code for each client's compliance needs, rubrics are expressed as configuration: label definitions, scoring scales, failure categories, escalation thresholds, and required evidence fields. Taxonomy configs are version-controlled, so every review decision can be traced to the specific rubric version that applied. This makes pilots fast (days, not weeks) and prevents custom engineering for each new use case.",
    whyItMatters: "Taxonomy config is what makes the production gate adaptable without engineering. A financial services firm has different rules than a B2B SaaS company. Instead of rebuilding, you reconfigure. Version stamping ensures audit traceability even as rules evolve.",
    howBookbagHelps: "Bookbag's taxonomy system lets you define rubrics, label sets, scoring criteria, and escalation rules as configuration. Dynamic annotation UIs adapt to the taxonomy. Every review decision references the taxonomy version used.",
    metaTitle: "Taxonomy Config for AI Review | Bookbag Intelligence",
    metaDescription: "Taxonomy config defines the rubrics and rules for AI content review. Learn how configurable taxonomies make review adaptable and auditable.",
    faqs: [
      { question: "What goes into a taxonomy config?", answer: "Label definitions (what each verdict means), scoring criteria (what triggers each verdict), required fields (evidence, rationale), escalation thresholds (when to route to SME), and any domain-specific rules." },
      { question: "Can we change the taxonomy mid-project?", answer: "Yes. Taxonomy changes are version-stamped. Old reviews reference the version that applied at the time. New reviews use the updated version. Full auditability across versions." },
      { question: "Do you provide template taxonomies?", answer: "Yes. Bookbag provides taxonomy templates for common use cases (FINRA compliance, B2B SaaS outbound, cold email quality). You customize these to your specific requirements." },
    ],
    relatedTerms: ["rubric-versioning", "ai-audit-trail", "qa-review-workflow"],
    relatedPersonas: ["compliance-officer", "revops"],
    relatedComparison: "bookbag-vs-internal-qa",
  },

  "rubric-versioning": {
    term: "Rubric Versioning",
    definition: "The practice of version-stamping review rubrics so every verdict can be traced to the specific rules that applied at the time of the decision.",
    fullDescription: "Rubric versioning means every revision of your review rubrics gets a unique version identifier. When a reviewer makes a decision, the rubric version is recorded alongside the verdict. This creates precise auditability: months later, you can see exactly which rules applied to any specific decision. Versioning also enables safe rubric evolution — you can update rules without invalidating prior decisions, because each decision references the version that was active at the time.",
    whyItMatters: "Compliance rubrics change as regulations evolve, products update, and best practices shift. Without versioning, you can't prove which rules applied to a specific message review. Regulators expect this level of traceability for supervised communications.",
    howBookbagHelps: "Bookbag automatically version-stamps every taxonomy change. Review decisions include the rubric version in their audit record. You can compare decisions across versions to see how rule changes impacted verdicts.",
    metaTitle: "Rubric Versioning for AI Review | Bookbag Intelligence",
    metaDescription: "Rubric versioning stamps every review decision with the rules that applied. Learn why version-controlled rubrics matter for compliance and audit.",
    faqs: [
      { question: "Why is rubric versioning important for compliance?", answer: "Regulators want to know what rules applied to a specific decision. Version stamps provide this traceability. Without them, you can't prove which standards were in effect when a review was conducted." },
      { question: "What happens when we update a rubric?", answer: "A new version is created. All future reviews use the new version. All past reviews retain their reference to the old version. Nothing is overwritten or lost." },
      { question: "Can we roll back to a previous rubric version?", answer: "Yes. Activating a previous version creates a new version identifier that references the old rules. The version history remains intact for audit purposes." },
    ],
    relatedTerms: ["taxonomy-config", "ai-audit-trail", "audit-ready-review"],
    relatedPersonas: ["compliance-officer", "finserv"],
    relatedComparison: "bookbag-vs-manual-review",
  },

  "annotator-calibration": {
    term: "Annotator Calibration",
    definition: "The process of training and aligning human reviewers to apply rubrics consistently, measured through gold set evaluation and inter-annotator agreement metrics.",
    fullDescription: "Annotator calibration ensures that different human reviewers apply the same standards when evaluating AI output. This involves gold set testing (reviewing pre-labeled examples with known correct answers), rubric training sessions, ongoing quality sampling, and inter-annotator agreement metrics. Without calibration, two reviewers might give the same message different verdicts — undermining the reliability of the entire review process. Calibration is ongoing, not one-time, because standards evolve and reviewer consistency can drift.",
    whyItMatters: "Inconsistent review is worse than no review — it creates false confidence. If your safe_to_deploy verdicts depend on which reviewer happened to get the message, the gate isn't reliable. Calibration ensures every verdict is trustworthy regardless of reviewer.",
    howBookbagHelps: "Bookbag supports calibration through gold set management, quality sampling, and agreement tracking. New reviewers calibrate against gold sets before handling production items. Ongoing sampling catches consistency drift.",
    metaTitle: "Annotator Calibration for AI Review | Bookbag Intelligence",
    metaDescription: "Annotator calibration aligns human reviewers to apply consistent standards. Learn how gold sets, sampling, and agreement metrics ensure reliable AI review.",
    faqs: [
      { question: "What is a gold set?", answer: "A collection of pre-labeled examples with known correct answers, reviewed and approved by SMEs. New reviewers label gold set items to verify they apply rubrics correctly before handling production work." },
      { question: "How is calibration measured?", answer: "Through gold set agreement (reviewer vs. known answers), inter-annotator agreement (reviewer vs. reviewer on the same items), and QA sampling rates (errors caught in quality checks)." },
      { question: "How often should reviewers be recalibrated?", answer: "Initial calibration before production, then ongoing through quality sampling. Recalibration sessions when rubrics change, agreement drops, or new failure patterns emerge." },
    ],
    relatedTerms: ["qa-review-workflow", "taxonomy-config", "gold-standard-rewrites"],
    relatedPersonas: ["revops", "head-of-cs"],
    relatedComparison: "human-review-vs-automated-qa",
  },

  "audit-ready-review": {
    term: "Audit-Ready Review",
    definition: "A review process designed to produce documentation that satisfies regulatory examination, enterprise procurement, and compliance audit requirements from the start.",
    fullDescription: "Audit-ready review means designing the review process so that every output is suitable for regulatory or compliance examination without additional documentation work. This includes: attributable decisions (who reviewed), timestamped records (when), rubric citations (what standard applied), rationale (why the decision was made), evidence (supporting the rationale), and immutability (records can't be altered). The key principle is that audit readiness is built into the workflow, not added after the fact.",
    whyItMatters: "Retroactively documenting review decisions for audits is expensive, error-prone, and sometimes impossible. Building audit readiness into the process means you're always prepared for examination. For regulated industries, this can be the difference between passing and failing a regulatory review.",
    howBookbagHelps: "Every Bookbag decision automatically captures: reviewer identity, timestamp, rubric version, verdict, rationale, evidence, and corrections. Exports are immutable and tenant-isolated. The audit trail is a byproduct of normal operations, not extra work.",
    metaTitle: "Audit-Ready Review for AI Content | Bookbag Intelligence",
    metaDescription: "Audit-ready review builds compliance documentation into the AI review process. Every decision is attributable, timestamped, and evidence-backed.",
    faqs: [
      { question: "What makes a review 'audit-ready'?", answer: "Attributable (who), timestamped (when), rubric-referenced (what standard), rational (why), evidence-backed (proof), and immutable (can't be changed). All captured automatically during normal review operations." },
      { question: "Who typically audits AI review processes?", answer: "Regulatory examiners (FINRA, state regulators), enterprise procurement teams, internal compliance officers, and external auditors. Each may have different requirements, but the core elements are consistent." },
      { question: "Do we need to do extra work for audit readiness?", answer: "No. Bookbag captures audit-ready records automatically during normal review operations. The documentation is a byproduct of the workflow, not a separate task." },
    ],
    relatedTerms: ["ai-audit-trail", "rubric-versioning", "sme-escalation"],
    relatedPersonas: ["compliance-officer", "finserv"],
    relatedComparison: "bookbag-vs-manual-review",
  },
}

export function getGlossaryTerm(slug) {
  return glossaryTerms[slug] || null
}

export function getAllGlossarySlugs() {
  return Object.keys(glossaryTerms)
}
