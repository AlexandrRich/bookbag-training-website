export const glossaryTerms = {
  "ai-production-gate": {
    term: "AI Production Gate",
    category: "Verdicts",
    definition: "A checkpoint between AI content generation and customer delivery that routes every AI output through human-authority review lanes — safe to deploy, needs fix, or blocked — before it reaches recipients.",
    keyInsight: "Prompt engineering tells AI what to say. A production gate verifies what actually gets said.",
    fullDescription: "Think of it this way: your AI writes a message. Before it ships, it passes through a gate. A human reviewer evaluates it against your rules. The gate renders a verdict — safe_to_deploy, needs_fix, or blocked — and routes it accordingly. That's the AI Production Gate. It's not a filter. It's not a prompt tweak. It's a structured, auditable checkpoint with human authority at every level. Annotators handle routine review. QA reviewers fix flagged content. SMEs make final calls on blocked items with documented evidence. Every decision is logged. Every correction becomes training data. The gate doesn't just catch problems — it makes your AI smarter over time.",
    whyItMatters: "Here's the uncomfortable truth: your AI will hallucinate, violate compliance rules, and send off-brand messages. Not occasionally — regularly. Without a production gate, you find out when a prospect screenshots it and posts it on LinkedIn, or when a regulator sends a letter. A production gate catches those problems before delivery, documents the human oversight, and turns every correction into data that makes the AI better. It's the difference between hoping your AI behaves and proving it.",
    howBookbagHelps: "Bookbag built the production gate model specifically for outbound messaging. Every message gets a verdict. Every correction becomes training data. Every decision is logged with who decided, when, and why. When a regulator or procurement team asks 'who approved this?' — you hand them the immutable audit trail. The gate scales from hundreds to millions of messages per month, with authority escalation built in at every tier.",
    bookbagFeatures: [
      { title: "Three-verdict routing", description: "Every message gets one of three verdicts: safe_to_deploy, needs_fix, or blocked. No gray area." },
      { title: "Tiered human authority", description: "Annotators handle routine review. QA reviewers fix flagged items. SMEs make final calls on blocked content with evidence." },
      { title: "Immutable audit trail", description: "Every verdict, correction, and escalation is timestamped and attributed. When compliance asks, you have the receipts." },
    ],
    metaTitle: "What Is an AI Production Gate? | Bookbag Intelligence",
    metaDescription: "An AI Production Gate is a checkpoint between AI generation and delivery that routes every output through human review lanes. Learn how it works.",
    faqs: [
      { question: "How is an AI Production Gate different from prompt engineering?", answer: "Prompt engineering tries to prevent bad output at the generation step. An AI Production Gate catches what gets through after generation. Think of it this way: prompt engineering is the seatbelt, the production gate is the crash test. Both matter, but only the gate gives you documented proof — with human authority, an immutable audit trail, and verdicts attached to every message." },
      { question: "Does a production gate slow down delivery?", answer: "Not meaningfully. Messages that pass your rubric get the safe_to_deploy verdict and are cleared for delivery — no human touch needed. Only needs_fix and blocked items enter the review queue. After calibration, the majority of messages pass review quickly. The remainder that need human review are exactly the ones you want a human looking at." },
      { question: "What types of content can a production gate review?", answer: "Any text-based AI output: outbound emails, SMS messages, LinkedIn messages, call scripts, chat responses, and marketing copy. If your AI generates it and a customer sees it, it should pass through the gate." },
    ],
    relatedTerms: ["safe-to-deploy", "needs-fix", "blocked-verdict", "message-gating"],
    relatedPersonas: ["ai-sdr-vendors", "vp-product"],
    relatedComparison: "ai-production-gate-vs-prompt-guardrails",
  },

  "message-gating": {
    term: "Message Gating",
    category: "Verdicts",
    definition: "The process of evaluating and routing AI-generated messages through defined review lanes before they are sent to recipients.",
    fullDescription: "Message gating is borrowed from manufacturing — nothing ships without passing a quality checkpoint. Applied to AI outbound, it means every AI-generated message gets intercepted before delivery and evaluated against your rubric. The message receives a verdict: safe_to_deploy, needs_fix, or blocked. Safe messages flow through automatically. Flagged messages route to human reviewers with the right authority level. It's not a speed bump — it's a quality system. The safe lane keeps things fast. The fix and blocked lanes keep things safe. And every verdict creates an audit record and potential training data.",
    whyItMatters: "Without gating, your AI outbound is a black box. Messages go out, and you hope they're fine. Most are. But the ones that aren't — the hallucinated claim, the compliance violation, the off-brand joke — those find their way to someone's inbox and then to a screenshot. Gating means you catch those before delivery. You also get data on what's failing and why, which means you can actually fix the root cause instead of playing whack-a-mole.",
    howBookbagHelps: "Bookbag's gating system routes every message through three lanes: safe_to_deploy (approved after review, no delay), needs_fix (QA reviewer corrects and approves), and blocked (SME reviews with documented rationale). Every verdict carries rubric citations, reviewer identity, and timestamps. The gating data feeds directly into training data export — so every correction makes the next batch of messages better.",
    metaTitle: "What Is Message Gating? | Bookbag Intelligence",
    metaDescription: "Message gating evaluates and routes AI-generated messages through review lanes before delivery. Learn how gating prevents quality issues in AI outbound.",
    faqs: [
      { question: "What happens to a 'gated' message?", answer: "It gets evaluated against your rubric and receives a verdict. safe_to_deploy messages are cleared for delivery and ship immediately. needs_fix messages go to a QA reviewer who corrects them and approves the fixed version. blocked messages route to an SME with final authority who reviews with documented rationale. Every path produces an audit record." },
      { question: "How is gating different from spam filtering?", answer: "Spam filters evaluate what's coming in to you. Gating evaluates what's going out from you. It's proactive quality control on your own AI-generated messages — catching problems before your prospects ever see them. Totally different direction, totally different purpose." },
      { question: "Can gating handle high volume?", answer: "That's the whole point. The safe_to_deploy lane clears clean messages with zero delay. Only the messages that need human authority actually enter the review queue. Bookbag scales gating to millions of messages per month without bottlenecking delivery." },
    ],
    relatedTerms: ["ai-production-gate", "safe-to-deploy", "needs-fix"],
    relatedPersonas: ["revops", "ai-sdr-vendors"],
    relatedComparison: "bookbag-vs-prompt-engineering",
  },

  "safe-to-deploy": {
    term: "Safe to Deploy",
    category: "Verdicts",
    definition: "A verdict in the AI Production Gate indicating that an AI-generated message meets all quality, compliance, and brand standards and is approved for delivery without human review.",
    keyInsight: "Not 'probably fine' — safe_to_deploy means a human with authority reviewed it against your rubric and approved it.",
    fullDescription: "safe_to_deploy is the green light. It's the first of three verdict lanes in the AI Production Gate, and it means: this message passed every check in your rubric and can ship without a human touching it. No queue. No delay. But here's what matters — it's not unreviewed. It's evaluated against your configured rubric and logged with a verdict, timestamp, and rubric version. If an auditor asks about it six months later, you can show exactly what standard it was measured against and when. The goal of the whole system is to maximize safe_to_deploy rates through better AI, better rubrics, and better training data — because every safe message is throughput you don't have to pay a human to touch.",
    whyItMatters: "safe_to_deploy rates are your north star metric for AI outbound efficiency. Low safe rates mean your gate is a bottleneck — too many messages need human review. High safe rates mean your AI is well-calibrated, your rubrics are dialed in, and your review costs are dropping. Teams typically see safe rates climb steadily after calibrating with real correction data. That's the flywheel working.",
    howBookbagHelps: "Bookbag's safe_to_deploy lane clears messages that pass your rubric — zero human delay. But every safe message is still logged with full provenance: which rubric version applied, when the verdict was rendered, what the message contained. QA can sample safe messages anytime for calibration checks. And as your team exports corrections as training data and retrains models, safe rates climb over time. That's less review cost, faster delivery, and a growing immutable audit trail.",
    bookbagFeatures: [
      { title: "Zero-delay clearance", description: "Messages meeting all rubric criteria get safe_to_deploy instantly — no queue, no human wait time, no bottleneck." },
      { title: "Full audit logging", description: "Every safe message is logged with its verdict, rubric version, and timestamp. Auditable anytime, even months later." },
      { title: "Calibration sampling", description: "QA reviewers can sample safe messages to verify the rubric isn't letting bad content through. Catches drift before it becomes a problem." },
    ],
    metaTitle: "What Is Safe to Deploy? | Bookbag Intelligence",
    metaDescription: "Safe to deploy is a production gate verdict meaning an AI message meets all standards and is approved for delivery. Learn how it works.",
    faqs: [
      { question: "Does safe_to_deploy mean no human ever sees the message?", answer: "It means no human needs to see it for approval — it cleared your rubric automatically. But it's still logged, and QA reviewers can sample safe messages anytime for calibration. Think of it as 'approved by your rules' rather than 'unsupervised.' The immutable audit trail captures it either way." },
      { question: "What percentage of messages are typically safe?", answer: "After initial calibration, the majority of messages typically receive the safe_to_deploy verdict. Regulated industries often start lower because their rubrics are stricter. But as corrections from needs_fix items become training data, safe rates climb steadily. That's the flywheel doing its job." },
      { question: "Can safe messages be audited later?", answer: "Absolutely. Every safe_to_deploy message is in the immutable audit trail with its verdict, the rubric version that applied, and the timestamp. Regulators, procurement teams, compliance officers — anyone can pull the record and see exactly what standard the message was measured against." },
    ],
    relatedTerms: ["needs-fix", "blocked-verdict", "ai-production-gate"],
    relatedPersonas: ["revops", "compliance-officer"],
    relatedComparison: "bookbag-vs-manual-review",
  },

  "needs-fix": {
    term: "Needs Fix",
    category: "Verdicts",
    definition: "A verdict in the AI Production Gate indicating that an AI-generated message has quality or compliance issues that a QA reviewer can correct before the message is approved for delivery.",
    keyInsight: "needs_fix is where the magic happens — every correction is simultaneously a quality save AND a training data point that makes the AI smarter.",
    fullDescription: "needs_fix is the middle verdict — the message isn't clean enough to ship, but it's not bad enough to block. It has fixable issues: a tone that's too aggressive, a claim that's slightly off, personalization that missed the mark, compliance language that needs tightening. The message routes to a QA reviewer who sees exactly what was flagged, makes the correction, and approves the fixed version. Here's the key insight: that correction is doing double duty. The fixed message goes to the recipient. And the pair — original (bad) plus correction (good) — becomes training data you can export to make your AI better. Every needs_fix item is both a quality save and an investment in future quality.",
    whyItMatters: "The needs_fix lane is the most valuable part of the entire AI Production Gate. Safe messages are great but teach you nothing. Blocked messages are rare. needs_fix is where you find the patterns — the recurring tone issues, the systematic hallucinations, the compliance gaps your AI keeps hitting. Fix the message, capture the correction, export the training data, retrain the model. That's how safe_to_deploy rates climb from 60% to 85%.",
    howBookbagHelps: "Bookbag routes needs_fix messages to QA reviewers with full context: the original message, the specific rubric violations flagged, and citation of which rules were triggered. Reviewers correct in the annotation interface with gold standard rewrites. Every correction is automatically structured as exportable SFT or DPO training data — original (rejected) paired with the correction (preferred). The immutable audit trail captures who reviewed, what changed, and why.",
    bookbagFeatures: [
      { title: "Contextual QA interface", description: "Reviewers see the flagged issues, rubric citations, and original message side-by-side. No guessing about what needs fixing or why." },
      { title: "Automatic training data structuring", description: "Every correction is automatically formatted as SFT and DPO pairs — ready to export for model fine-tuning without additional processing." },
      { title: "Pattern detection", description: "Aggregate needs_fix data reveals systematic AI weaknesses — recurring tone issues, common hallucination types, persistent compliance gaps — so you can fix root causes." },
    ],
    metaTitle: "What Is a Needs Fix Verdict? | Bookbag Intelligence",
    metaDescription: "Needs fix is a production gate verdict meaning an AI message has fixable issues. Learn how QA review corrects AI outbound and generates training data.",
    faqs: [
      { question: "What types of issues trigger needs_fix?", answer: "The stuff a competent QA reviewer can handle: tone mismatches, minor factual errors, weak personalization, missing context, borderline compliance language, formatting issues. Anything that needs correction but doesn't require SME-level domain expertise or authority escalation." },
      { question: "How long does the fix process take?", answer: "QA reviewers typically turn around needs_fix items in minutes, not hours. The reviewer sees exactly what's flagged, makes the correction, and approves. Both the original and the fix are logged in the immutable audit trail. Turnaround depends on queue volume and your SLA tier." },
      { question: "Can needs_fix corrections improve the AI?", answer: "That's the whole point. Every correction creates a training pair: the original message (what the AI got wrong) and the gold standard rewrite (what it should have said). Export those as SFT or DPO data, retrain your model, and watch your safe_to_deploy rate climb. The corrections literally teach the AI your standards." },
    ],
    relatedTerms: ["safe-to-deploy", "blocked-verdict", "qa-review-workflow"],
    relatedPersonas: ["head-of-cs", "sales-enablement"],
    relatedComparison: "rewrite-workflow-vs-prompt-tweaks",
  },

  "blocked-verdict": {
    term: "Blocked Verdict",
    category: "Verdicts",
    definition: "A verdict in the AI Production Gate indicating that an AI-generated message has serious issues requiring SME (subject matter expert) review, rationale, and evidence before any decision is made.",
    keyInsight: "blocked isn't a rejection — it's an authority escalation. The message doesn't ship until someone with the right expertise and authority makes a documented call.",
    fullDescription: "blocked is the red flag. It means the AI-generated message has issues serious enough that a QA reviewer doesn't have the authority to resolve them. We're talking prohibited claims, compliance violations, potential legal exposure, severe hallucinations — the kind of stuff that ends careers if it ships. Blocked messages route to a designated SME: a compliance officer, a clinical expert, a legal reviewer, someone with domain authority. The SME reviews the message with full context, makes a call (approve with corrections or reject entirely), and documents their rationale with evidence citations. That decision, that rationale, that evidence — it all goes into the immutable audit trail. This is your organization's proof that the hard calls got the right level of human authority.",
    whyItMatters: "The blocked lane is your insurance policy against catastrophic AI failures. Every AI system will occasionally generate something dangerous — a fabricated regulatory claim, a prohibited guarantee, a hallucinated product capability. The question isn't whether it happens, it's whether you catch it before delivery. In regulated industries, the blocked lane is often the single most important compliance control — it's documented proof that an expert with authority reviewed the high-risk content before it reached anyone.",
    howBookbagHelps: "Bookbag routes blocked messages to designated SMEs with full context: the original message, every flagged issue, rubric citations showing which rules triggered the block, and space for structured evidence and rationale. SMEs provide documented decisions through the authority escalation interface. The entire chain — flag, review, decision, rationale, evidence — is captured in the immutable audit trail. When regulators ask about your highest-risk AI outputs, this is the record you produce.",
    bookbagFeatures: [
      { title: "SME authority routing", description: "Blocked items go directly to designated subject matter experts — not general QA. The right expertise meets the right problem." },
      { title: "Structured rationale capture", description: "SMEs document their decision with rationale text, evidence citations, and rubric references. Not just a thumbs-up — a defensible record." },
      { title: "Full provenance chain", description: "From initial flag through SME decision, every step is timestamped, attributed, and immutable. The complete evidence trail for your highest-risk items." },
    ],
    metaTitle: "What Is a Blocked Verdict? | Bookbag Intelligence",
    metaDescription: "A blocked verdict routes high-risk AI messages to SME experts for review with evidence and rationale. Learn how it protects against compliance violations.",
    faqs: [
      { question: "What triggers a blocked verdict?", answer: "The serious stuff: prohibited claims, compliance violations, severe hallucinations, potential legal exposure, anything that exceeds QA reviewer authority. Your rubric configuration defines the specific triggers. If it could end up in a regulator's inbox or a lawsuit, it should probably trigger a block." },
      { question: "Who reviews blocked messages?", answer: "Designated SMEs with final human authority. In financial services, that might be a compliance officer. In healthcare, a clinical reviewer. In B2B SaaS, a senior product expert. You designate SMEs based on who has the domain expertise and organizational authority to make the call." },
      { question: "How much does a blocked verdict cost?", answer: "Blocked items consume 3 credits (vs. 1 for safe_to_deploy or needs_fix) because they require SME-level review with documented rationale and evidence. It's more expensive per item, but these are exactly the items where cutting corners would cost you orders of magnitude more." },
    ],
    relatedTerms: ["sme-escalation", "ai-audit-trail", "needs-fix"],
    relatedPersonas: ["compliance-officer", "finserv"],
    relatedComparison: "human-review-vs-automated-qa",
  },

  "sme-escalation": {
    term: "SME Escalation",
    category: "Verdicts",
    definition: "The process of routing high-risk AI-generated content to designated subject matter experts who have final authority to approve, correct, or reject the content with documented rationale.",
    keyInsight: "Authority escalation isn't about adding bureaucracy — it's about making sure the hard calls land on the desk of someone qualified to make them, with a paper trail that proves it.",
    fullDescription: "SME escalation is the authority escalation mechanism in the AI Production Gate. Here's how it works: a message gets a blocked verdict because it has issues too serious for a QA reviewer to handle — a prohibited claim, a compliance risk, something that needs domain expertise. The message routes to a designated subject matter expert. The SME sees the original message, the specific flags, the rubric citations, and has structured fields for their verdict, rationale, and evidence. They make the call: approve with corrections, or reject entirely. That decision — with all its documentation — becomes part of the immutable audit trail. It's a traceable decision by a qualified authority with explicit reasoning. That's what regulators and procurement teams are actually asking for when they say 'human oversight.'",
    whyItMatters: "When a regulator or enterprise procurement team asks 'who approved this AI output?' — the answer needs to be more than 'our system.' They want to see a named expert, a documented rationale, evidence citations, and a timestamp. Authority escalation provides exactly that. It's the difference between 'our AI handles it' (which makes compliance teams nervous) and 'a qualified expert reviewed and approved it with documented evidence' (which is defensible).",
    howBookbagHelps: "Bookbag provides a dedicated SME review interface built for authority escalation: full message context, rubric citations showing why the message was blocked, and structured fields for rationale and evidence. Every SME decision is timestamped, attributed, and captured in the immutable audit trail. You can designate different SMEs for different project types or compliance domains — so financial messages go to your compliance officer while product messages go to your senior PM.",
    bookbagFeatures: [
      { title: "Domain-specific SME routing", description: "Route blocked items to the right expert based on the type of issue — compliance, clinical, legal, product. Different problems, different authorities." },
      { title: "Structured evidence capture", description: "SMEs document their decision with rationale, evidence citations, and rubric references in structured fields — not free-text emails." },
      { title: "Immutable decision record", description: "Every authority escalation decision is timestamped, attributed, and permanently recorded. The strongest possible audit documentation for your highest-risk items." },
    ],
    metaTitle: "What Is SME Escalation in AI Quality? | Bookbag Intelligence",
    metaDescription: "SME escalation routes high-risk AI content to subject matter experts for final authority decisions. Learn how escalation provides audit-ready compliance proof.",
    faqs: [
      { question: "Who qualifies as an SME?", answer: "Whoever has the domain expertise and organizational authority to make the call. In financial services, a compliance officer. In healthcare, a clinical reviewer. In B2B SaaS, a senior product marketer or legal counsel. You designate SMEs based on your specific authority requirements — Bookbag routes to them automatically." },
      { question: "How does SME review differ from QA review?", answer: "It's about authority level. QA reviewers handle needs_fix items — correctable issues within their expertise. SMEs handle blocked items — high-risk issues that require domain expertise, organizational authority, and documented rationale. A QA reviewer fixes a tone issue. An SME decides whether a compliance-sensitive claim can ship." },
      { question: "Is SME rationale part of the audit trail?", answer: "Every word of it. The verdict, the rationale text, evidence citations, the rubric version that applied, and the SME's identity — all captured in the immutable audit trail. When a regulator asks about a specific decision, you can produce the complete record of who decided, what they decided, and why." },
    ],
    relatedTerms: ["blocked-verdict", "ai-audit-trail", "rubric-versioning"],
    relatedPersonas: ["compliance-officer", "finserv"],
    relatedComparison: "ai-outbound-compliance-vs-legal-review",
  },

  "finra-ai-compliance": {
    term: "FINRA AI Compliance",
    category: "Compliance",
    definition: "The application of FINRA advertising and communications rules (particularly Rule 2210) to AI-generated financial services outbound, ensuring AI content meets the same regulatory standards as human-authored communications.",
    keyInsight: "FINRA doesn't care if a human or an AI wrote the message. They care whether a qualified human supervised it — and whether you can prove it.",
    fullDescription: "Here's the reality for financial services firms using AI outbound: FINRA Rule 2210 applies to your AI-generated content the same way it applies to content written by a human rep. No exception. No grace period. Your AI will generate performance guarantees, promissory language, and unsubstantiated claims — because that's what language models do when they're optimizing for persuasive sales copy. The question is whether you catch it before the prospect sees it, or whether FINRA catches it after. An AI Production Gate with FINRA-specific rubrics flags these violations before delivery. Blocked items route to your compliance SMEs through authority escalation. Every decision is documented in the immutable audit trail. When the examiner asks, you show the trail.",
    whyItMatters: "FINRA standard review fees are $300 per filing ($600 expedited). FTC penalties can exceed $53,000 per violation. A single AI-generated email with a prohibited performance claim can trigger a regulatory examination. At AI scale, you're not sending one risky message — you're potentially sending thousands. Systematic pre-send review with documented human authority isn't optional anymore. It's how you stay in business.",
    howBookbagHelps: "Bookbag provides FINRA-aware rubric templates that flag the specific violations examiners look for: performance guarantees, promissory language, omitted risk disclosures, unsubstantiated claims, and testimonials without disclaimers. Messages with these violations get a blocked verdict and route to your compliance SMEs through authority escalation. Every decision is captured in the immutable audit trail with timestamped provenance — exactly what FINRA expects to see during an examination.",
    bookbagFeatures: [
      { title: "FINRA-specific rubric templates", description: "Pre-built rules targeting the exact violations FINRA examiners look for: performance guarantees, promissory language, missing disclosures, unsubstantiated claims." },
      { title: "Compliance SME routing", description: "Blocked items route directly to your compliance officers through authority escalation — the people with the expertise and authority FINRA expects to be supervising." },
      { title: "Examination-ready audit trail", description: "Every verdict, correction, and SME decision is timestamped and attributed in the immutable audit trail. Hand the examiner the records, not a narrative." },
    ],
    metaTitle: "FINRA AI Compliance for Outbound | Bookbag",
    metaDescription: "Learn how FINRA rules apply to AI-generated financial services outbound. Catch performance claims, missing disclosures, and prohibited language before delivery.",
    faqs: [
      { question: "Does FINRA specifically address AI-generated content?", answer: "FINRA hasn't created separate AI rules because they don't need to. Their communications rules apply to all firm communications regardless of how they're created. If your AI writes an email to a prospect, that email is subject to the same supervision requirements as one written by your registered rep. Period." },
      { question: "What are the most common AI violations of FINRA 2210?", answer: "Performance guarantees are the big one — AI loves to generate impressive-sounding return claims. Also: promissory language, omitted risk disclosures, testimonials without disclaimers, and unsubstantiated claims about investment products. Basically, everything that makes sales copy persuasive is exactly what FINRA prohibits without proper disclosures." },
      { question: "Can Bookbag replace our FINRA compliance review process?", answer: "No, and that's by design. Bookbag systematizes and documents your compliance review — rubric enforcement, authority escalation to your compliance SMEs, and an immutable audit trail. Your compliance team retains human authority and oversight. Bookbag makes their job scalable and auditable, not obsolete." },
    ],
    relatedTerms: ["ai-outbound-compliance", "ai-audit-trail", "sme-escalation"],
    relatedPersonas: ["finserv", "compliance-officer"],
    relatedComparison: "ai-outbound-compliance-vs-legal-review",
  },

  "ai-outbound-compliance": {
    term: "AI Outbound Compliance",
    category: "Compliance",
    definition: "The practice of ensuring AI-generated outbound communications meet applicable legal, regulatory, and industry standards before delivery to recipients.",
    fullDescription: "AI outbound compliance is the practice of making sure every AI-generated message meets legal, regulatory, and industry standards before a prospect sees it. That means CAN-SPAM for email, TCPA for texts and calls, FINRA 2210 for financial services, HIPAA for healthcare, FDCPA for collections, plus state-level advertising laws. Here's the challenge: a human writes one email at a time. AI generates thousands per hour. Traditional compliance review — someone on legal reading emails in a queue — doesn't scale to that volume. You need a system. An AI Production Gate with compliance-specific rubrics, three-verdict routing (safe_to_deploy, needs_fix, blocked), authority escalation to compliance SMEs, and an immutable audit trail that documents every decision. That's systematic compliance at AI scale.",
    whyItMatters: "Every AI-generated message carries compliance risk. Multiply that by thousands of messages per day and you see the problem. Your compliance team can't manually review that volume. But regulators don't care about your volume problem — they care whether you supervised the content. The answer is a structured system: rules encoded as rubrics, human authority at the right levels, and documentation that proves you did the work. Without it, you're gambling at scale.",
    howBookbagHelps: "Bookbag provides the systematic review infrastructure that makes AI outbound compliance possible at scale. Your compliance team defines the rules as rubric configurations. Every message gets evaluated against those rules. Clean messages get safe_to_deploy. Fixable issues go to QA reviewers. Serious violations route to compliance SMEs through authority escalation. Every decision — every single one — is captured in the immutable audit trail with who decided, when, and why. That's the supervision documentation regulators expect to see.",
    metaTitle: "AI Outbound Compliance Guide | Bookbag",
    metaDescription: "AI outbound compliance ensures AI-generated messages meet legal and regulatory standards. Learn the requirements and how to implement compliance review.",
    faqs: [
      { question: "What regulations apply to AI outbound?", answer: "It depends on your industry and channels, but the big ones: CAN-SPAM (email), TCPA (texts and calls), FTC Act (deceptive practices). Industry-specific: FINRA 2210 (financial services), HIPAA (healthcare), FDCPA (collections). Plus state-level advertising and consumer protection laws. If a human would need to comply with it, your AI does too." },
      { question: "Is AI-generated content treated differently by regulators?", answer: "No, and that's the key point. Regulators hold firms to the same standards regardless of whether a human or an AI wrote the message. What they want to see is documented supervision — proof that qualified humans reviewed the content with defined standards. An AI Production Gate provides exactly that." },
      { question: "How does Bookbag help with compliance documentation?", answer: "Every message, verdict, correction, and SME decision is logged in the immutable audit trail with timestamps, reviewer identity, rubric version, and rationale. When a regulator asks 'how do you supervise your AI-generated communications?' — you hand them the trail. It's not a narrative about your process; it's the actual record of every decision." },
    ],
    relatedTerms: ["finra-ai-compliance", "can-spam-ai-messaging", "tcpa-ai-compliance"],
    relatedPersonas: ["compliance-officer", "finserv"],
    relatedComparison: "ai-outbound-compliance-vs-legal-review",
  },

  "can-spam-ai-messaging": {
    term: "CAN-SPAM for AI Messaging",
    category: "Compliance",
    definition: "The application of the CAN-SPAM Act requirements to AI-generated commercial email, including truthful headers, honest subject lines, physical address inclusion, and opt-out mechanisms.",
    fullDescription: "CAN-SPAM applies to every commercial email your AI sends — no exceptions. The rules are straightforward: truthful header information, non-deceptive subject lines, identification as advertising when applicable, a valid physical address, and a clear opt-out mechanism. The problem? AI doesn't know these rules exist. It's optimizing for engagement, not compliance. It'll write a subject line that technically misleads. It'll generate body copy that omits your physical address. It'll craft a message that doesn't include an opt-out link. Each of those is a separate CAN-SPAM violation. At AI scale, where you're generating hundreds or thousands of emails, the exposure adds up fast. An AI Production Gate with CAN-SPAM rubrics catches these before delivery — flagging missing elements as needs_fix and routing serious violations through authority escalation.",
    whyItMatters: "CAN-SPAM violations carry penalties up to $51,744 per email. Per email. Your AI can generate a thousand non-compliant emails in the time it takes someone to notice the first one. Without pre-send review, you're accumulating exposure at machine speed. Systematic gating with CAN-SPAM-specific rubrics is the only way to prevent mass violations when AI is writing your outbound.",
    howBookbagHelps: "Bookbag lets you configure rubrics that check for specific CAN-SPAM requirements: honest subject lines, sender identification, physical address presence, opt-out mechanism, and content accuracy. Missing elements trigger needs_fix for correction. Deceptive content triggers blocked for authority escalation. Every verdict is logged in the immutable audit trail — proof that you checked before sending.",
    metaTitle: "CAN-SPAM Compliance for AI Email | Bookbag Intelligence",
    metaDescription: "Learn how CAN-SPAM requirements apply to AI-generated emails. Catch missing opt-outs, deceptive subjects, and compliance gaps before sending.",
    faqs: [
      { question: "Can AI accidentally violate CAN-SPAM?", answer: "All the time. AI optimizes for engagement, not compliance. It'll write subject lines that inadvertently mislead, generate copy that omits required identification, or create messages without opt-out mechanisms. These aren't edge cases — they're what happens when a language model writes sales emails without compliance guardrails." },
      { question: "What's the penalty for CAN-SPAM violations?", answer: "Up to $51,744 per individual email. Not per campaign — per email. If your AI sends 500 non-compliant emails before someone catches it, you're looking at over $25 million in potential exposure. That math alone justifies an AI Production Gate." },
      { question: "How does Bookbag check for CAN-SPAM compliance?", answer: "You configure rubrics that flag the specific requirements: deceptive subject lines, missing sender identification, absent physical address, missing opt-out mechanism, misleading content claims. Each violation type gets appropriate routing — needs_fix for missing elements that QA can add, blocked for deceptive content that needs authority escalation." },
    ],
    relatedTerms: ["ai-outbound-compliance", "tcpa-ai-compliance", "outbound-deliverability-risk"],
    relatedPersonas: ["cold-email-infrastructure", "compliance-officer"],
    relatedComparison: "quality-gate-vs-deliverability-tooling",
  },

  "tcpa-ai-compliance": {
    term: "TCPA AI Compliance",
    category: "Compliance",
    definition: "Ensuring AI-generated text messages, calls, and voicemails comply with the Telephone Consumer Protection Act's consent, timing, and content requirements.",
    fullDescription: "The TCPA regulates phone-based communications: calls, texts, and voicemails. If your AI is generating content for any of these channels, TCPA applies — prior express consent requirements, time-of-day restrictions, caller identification, and opt-out provisions. AI introduces a specific flavor of risk here: it generates messages fast without awareness of consent status, time zones, or disclosure requirements. Your sending infrastructure handles some of this (consent databases, time-of-day logic), but the message content itself — the actual words — needs to meet TCPA standards too. That's where an AI Production Gate with TCPA-specific rubrics comes in: flagging content-level violations before delivery.",
    whyItMatters: "TCPA violations carry statutory damages of $500-$1,500 per message. Not per campaign. Per message. Class action lawsuits for TCPA violations regularly result in multi-million dollar settlements. Now imagine AI generating thousands of texts with a missing identification or a faulty opt-out disclosure. The per-message damages at AI scale are existential. Pre-send content review is the layer your sending infrastructure doesn't provide.",
    howBookbagHelps: "Bookbag evaluates AI-generated message content against your TCPA-aware rubrics — checking for missing identification, opt-out mechanism language, required disclosures, and compliance language issues. Content violations trigger needs_fix or blocked verdicts depending on severity. Authority escalation routes serious issues to your compliance SMEs. Every decision is captured in the immutable audit trail. (Note: consent verification and timing logic are typically handled by your sending infrastructure — Bookbag focuses on content compliance.)",
    metaTitle: "TCPA Compliance for AI Messaging | Bookbag Intelligence",
    metaDescription: "Learn how TCPA rules apply to AI-generated texts and calls. Prevent consent violations, timing issues, and missing disclosures with AI message review.",
    faqs: [
      { question: "Does the TCPA apply to AI-generated texts?", answer: "Yes. The TCPA applies to all commercial texts regardless of who or what wrote them. AI-generated texts must meet the same consent, timing, and content requirements as manually composed ones. The technology that creates the message doesn't change the regulatory obligation." },
      { question: "What's the biggest TCPA risk with AI?", answer: "Scale. AI generates thousands of texts quickly. If even a small percentage have content compliance issues — missing identification, faulty opt-out language, incomplete disclosures — the per-message statutory damages ($500-$1,500 each) add up to catastrophic exposure. Class action lawyers specifically look for systematic violations at volume." },
      { question: "Can Bookbag prevent TCPA violations?", answer: "Bookbag handles the content side: evaluating AI-generated message text against your TCPA rubrics for missing identification, opt-out language, and required disclosures. Content violations get flagged and routed through the AI Production Gate. Consent verification and timing rules are typically handled by your sending infrastructure — Bookbag is the content quality layer that sits on top." },
    ],
    relatedTerms: ["ai-outbound-compliance", "can-spam-ai-messaging", "ai-audit-trail"],
    relatedPersonas: ["lending", "collections"],
    relatedComparison: "ai-outbound-compliance-vs-legal-review",
  },

  "ai-audit-trail": {
    term: "AI Audit Trail",
    category: "Compliance",
    definition: "A complete, immutable record of every decision made about AI-generated content — including who reviewed it, when, which rubric applied, the verdict, rationale, and any corrections.",
    keyInsight: "An immutable audit trail turns 'we review our AI output' from a claim into a provable fact. Every decision, every reviewer, every timestamp — the receipts.",
    fullDescription: "An immutable audit trail is the complete, unalterable record of everything that happens to an AI-generated message in the AI Production Gate. For every message, it captures: the original AI output, which rubric version was applied, who reviewed it, when they reviewed it, what verdict they gave (safe_to_deploy, needs_fix, or blocked), what corrections were made, the rationale for the decision, and any evidence citations. The 'immutable' part is critical — once a record is logged, nobody can change it. Not the reviewer, not an admin, not Bookbag. And records are tenant-isolated, meaning one organization's audit data is never accessible to another. This isn't just good practice. For regulated industries, it's the difference between passing and failing a compliance examination.",
    whyItMatters: "When a regulator asks 'how do you supervise your AI-generated communications?' — there are two answers. The bad one: 'We have a process.' The good one: 'Here's the record of every decision, who made it, what standard applied, and when.' The immutable audit trail is the second answer. It turns compliance from a narrative into evidence. Enterprise procurement teams, regulators, and legal counsel increasingly require this level of documentation. Without it, your 'human oversight' claim is unverifiable.",
    howBookbagHelps: "Bookbag generates the immutable audit trail automatically — it's a byproduct of normal operations, not extra work. Every message that passes through the AI Production Gate gets a record with full provenance: message content, rubric version, reviewer identity, verdict, corrections, rationale, evidence citations, and timestamp. Records are immutable (can't be altered after logging) and tenant-isolated (your data stays yours). When you need to produce records for an audit, examination, or procurement review, the trail is already there.",
    bookbagFeatures: [
      { title: "Automatic record generation", description: "Every verdict creates an audit record automatically. No extra documentation work. The trail is a byproduct of the review workflow." },
      { title: "True immutability", description: "Once logged, records cannot be altered by anyone — reviewers, admins, or Bookbag itself. The integrity of the supervision record is absolute." },
      { title: "Full provenance chain", description: "Each record captures the complete decision chain: who reviewed, what standard applied, what verdict was given, what corrections were made, and why. The complete evidence trail." },
    ],
    metaTitle: "What Is an AI Audit Trail? | Bookbag Intelligence",
    metaDescription: "An AI audit trail documents every review decision on AI content — who, when, what, and why. Learn why audit trails matter for AI governance and compliance.",
    faqs: [
      { question: "What does an audit trail record include?", answer: "Everything: the original AI message, the rubric version that was applied, the reviewer's identity, the timestamp, the verdict (safe_to_deploy / needs_fix / blocked), any corrections made, the rationale for the decision, evidence citations, and the full approval chain. It's the complete provenance of the decision." },
      { question: "Can audit trail records be modified?", answer: "No. That's the 'immutable' part. Once a decision is logged in the Bookbag audit trail, it cannot be altered — not by the reviewer, not by an admin, not by anyone. If a decision needs to be revisited, a new record is created. The original stands forever." },
      { question: "Who typically needs audit trail access?", answer: "Compliance officers reviewing supervision practices. Risk teams assessing AI governance. Legal counsel preparing for examinations. Regulatory examiners during audits. Enterprise procurement teams evaluating vendors. Bookbag provides role-based access controls so the right people see the right records." },
    ],
    relatedTerms: ["sme-escalation", "rubric-versioning", "audit-ready-review"],
    relatedPersonas: ["compliance-officer", "finserv"],
    relatedComparison: "bookbag-vs-manual-review",
  },

  "ai-hallucination-detection": {
    term: "AI Hallucination Detection",
    category: "Training Data",
    definition: "The process of identifying factually incorrect, fabricated, or unsubstantiated claims in AI-generated content before it reaches recipients.",
    keyInsight: "AI hallucinations in outbound aren't just embarrassing — they're potential fraud. Your AI will confidently fabricate product features, case studies, and statistics. The question is whether you catch it before the prospect does.",
    fullDescription: "AI hallucination detection is catching your AI when it makes stuff up — and it will. Language models generate plausible-sounding content that can be completely fabricated: product features that don't exist, integrations you've never built, statistics pulled from nowhere, case studies that never happened. In outbound messaging, this is especially dangerous because hallucinations arrive in professional-looking emails that recipients have no reason to question. Automated tools can catch some obvious factual errors, but the subtle ones — the plausible-sounding but wrong claims — require human reviewers comparing AI output against your actual product facts, pricing, and approved messaging. That's what rubric-driven review in an AI Production Gate provides: human authority checking AI claims against ground truth.",
    whyItMatters: "A hallucinated product claim in a sales email doesn't just embarrass you — it can create legal liability. Tell a prospect your product integrates with their stack when it doesn't, or cite a performance metric you can't back up, and you're in breach-of-contract territory before the deal even closes. In regulated industries, hallucinations can constitute fraud or misrepresentation. Every correction of a hallucination also becomes training data that teaches your AI what's actually true about your product.",
    howBookbagHelps: "Bookbag reviewers evaluate AI messages against your configured rubrics, which include your approved product facts, pricing, features, integrations, and claims. When the AI fabricates something, the reviewer catches it. Minor hallucinations (wrong feature name, slightly off pricing) get needs_fix for correction. Serious fabrications (nonexistent products, false compliance claims) get blocked for authority escalation to SMEs. Every caught hallucination and its correction becomes exportable training data — teaching your AI what's actually true.",
    bookbagFeatures: [
      { title: "Fact-checking rubrics", description: "Configure rubrics with your approved product facts, features, pricing, and claims. Reviewers check AI output against ground truth, not just vibes." },
      { title: "Severity-based routing", description: "Minor hallucinations (wrong feature name) go to QA as needs_fix. Serious fabrications (false compliance claims) get blocked for SME authority escalation." },
      { title: "Hallucination-to-training pipeline", description: "Every caught hallucination and its correction becomes SFT/DPO training data. The AI literally learns what's true about your product from its mistakes." },
    ],
    metaTitle: "AI Hallucination Detection in Outbound | Bookbag",
    metaDescription: "Detect AI hallucinations in outbound messages before delivery. Catch fabricated claims, wrong facts, and unsubstantiated statements with human review.",
    faqs: [
      { question: "What are common hallucinations in outbound AI?", answer: "The greatest hits: fabricated product features, incorrect pricing, nonexistent integrations, made-up statistics, wrong company information about the recipient, and unsubstantiated performance claims. Basically, anything the AI thinks sounds persuasive, whether or not it's true." },
      { question: "Can automated tools catch hallucinations?", answer: "Some of them. Obvious factual errors, sure. But the dangerous hallucinations are the subtle ones — the claim that sounds plausible but is wrong, the integration that seems like it should exist but doesn't, the statistic that's in the right ballpark but fabricated. Those require human domain expertise to catch reliably." },
      { question: "How does Bookbag handle hallucinations?", answer: "Reviewers compare AI output against your rubrics — approved facts, features, pricing, claims. Hallucinations get flagged and corrected with gold standard rewrites. The corrections become training data for model improvement. And patterns in hallucination types reveal systematic prompt or model weaknesses you can fix at the root." },
    ],
    relatedTerms: ["ai-message-quality", "ai-brand-safety", "gold-standard-rewrites"],
    relatedPersonas: ["b2b-saas", "vp-product"],
    relatedComparison: "bookbag-vs-prompt-engineering",
  },

  "ai-message-quality": {
    term: "AI Message Quality",
    category: "Training Data",
    definition: "The overall standard of AI-generated outbound communications measured across dimensions including accuracy, tone, compliance, personalization relevance, and conversion effectiveness.",
    fullDescription: "AI message quality isn't a single number — it's a composite of how well your AI-generated outbound performs across multiple dimensions simultaneously. Is the message factually accurate? Is the tone right for this recipient? Does it comply with applicable regulations? Is the personalization relevant, or does it feel creepy and generic? Will it actually drive the intended action? Quality measurement requires structured evaluation against defined rubrics — not someone skimming messages and saying 'looks fine.' When you measure quality systematically through an AI Production Gate, you get data: safe_to_deploy / needs_fix / blocked rates, failure category breakdowns, trend lines over time. That data tells you exactly where your AI is strong and where it's failing.",
    whyItMatters: "Message quality hits everything at once. Low quality damages deliverability (spam complaints), conversion rates (bad personalization), brand perception (off-tone messaging), and compliance standing (regulatory violations) — all simultaneously. And you can't improve what you don't measure. Without systematic quality evaluation, you're guessing. With it, you know exactly which dimensions need work and can track improvement as your AI gets retrained with correction data.",
    howBookbagHelps: "Bookbag evaluates every AI message against your configurable quality rubrics covering accuracy, tone, compliance, and effectiveness. The AI Production Gate produces structured quality metrics: safe_to_deploy / needs_fix / blocked rates, failure category breakdowns, and trend data over time. Every correction from the review process becomes exportable training data (SFT, DPO) that improves the specific quality dimensions where your AI is weakest. Executive dashboards show quality trends so leadership can see the flywheel working.",
    metaTitle: "AI Message Quality: Metrics & Improvement | Bookbag",
    metaDescription: "AI message quality measures accuracy, tone, compliance, and effectiveness. Learn how to systematically measure and improve AI outbound quality.",
    faqs: [
      { question: "How do you measure AI message quality?", answer: "Through structured rubric evaluation across defined dimensions: factual accuracy, tone appropriateness, compliance adherence, personalization relevance, and conversion effectiveness. Each dimension has specific criteria in your rubric. The AI Production Gate evaluates every message against all dimensions and produces verdict data you can analyze." },
      { question: "What's a 'good' quality rate?", answer: "After calibration, most teams aim for the majority of their AI messages to receive the safe_to_deploy verdict — meaning they meet all quality standards and are approved by reviewers. Starting rates vary by industry and rubric strictness. The gap between starting and target is where the training data flywheel does its work." },
      { question: "How does quality improve over time?", answer: "Every needs_fix and blocked correction becomes training data. Export SFT and DPO pairs, retrain your models, and watch quality climb. Most teams see 10-20% improvement in safe_to_deploy rates over the first 3-6 months of systematic gating. The corrections literally teach the AI your quality standards." },
    ],
    relatedTerms: ["ai-production-gate", "gold-standard-rewrites", "qa-review-workflow"],
    relatedPersonas: ["revops", "sales-enablement"],
    relatedComparison: "bookbag-vs-internal-qa",
  },

  "outbound-deliverability-risk": {
    term: "Outbound Deliverability Risk",
    category: "Compliance",
    definition: "The risk that AI-generated outbound messages damage sender reputation, trigger spam filters, or reduce inbox placement rates.",
    fullDescription: "Outbound deliverability risk is what happens when your AI writes emails that look like spam — even if they aren't. AI can generate content that triggers spam filters (spammy language patterns, excessive links, formatting red flags), generates recipient complaints (irrelevant personalization, aggressive tone), or violates bulk sender requirements (the Gmail/Yahoo rules around authentication, unsubscribe, and spam rate thresholds). The damage is asymmetric: one bad batch of AI-generated emails can tank your sender reputation, and domain rehabilitation takes weeks or months. You don't get a warning. Your open rates just crater and you start piecing together what happened. Pre-send review through an AI Production Gate catches deliverability risk factors before they reach inboxes.",
    whyItMatters: "Gmail and Yahoo's bulk sender rules made deliverability existential. If your spam complaint rate exceeds 0.3%, you start losing inbox placement. For AI outbound vendors, deliverability failures cause customer churn. For enterprises, they halt revenue-generating campaigns. AI generates content fast enough to damage deliverability before anyone notices — which means pre-send gating isn't optional, it's infrastructure protection.",
    howBookbagHelps: "Bookbag rubrics flag deliverability risk factors before messages ship: spammy language patterns, excessive link density, missing compliance elements (unsubscribe, sender identification), aggressive tone likely to generate complaints, and formatting issues that trigger spam filters. Risky messages get needs_fix for correction. The AI Production Gate becomes a deliverability protection layer — catching the content issues that your sending infrastructure can't evaluate.",
    metaTitle: "Outbound Deliverability Risk from AI | Bookbag Intelligence",
    metaDescription: "AI-generated outbound can damage deliverability. Learn how to identify and prevent AI content that triggers spam filters, complaints, and inbox placement drops.",
    faqs: [
      { question: "How does AI content hurt deliverability?", answer: "Multiple ways: spam-trigger language patterns that filters catch, excessive links that look like phishing, aggressive tone that generates recipient complaints, missing compliance elements (unsubscribe, sender ID), and formatting quirks that raise red flags. AI optimizes for engagement, not deliverability — which means it'll write exactly the kind of content that gets flagged." },
      { question: "Can Bookbag check deliverability risk?", answer: "Yes. Configure rubrics that flag known deliverability risk factors in AI-generated content. Messages with deliverability issues route to needs_fix for correction before sending. Think of it as a content-level deliverability check that sits on top of your sending infrastructure's technical checks." },
      { question: "How long does it take to recover from deliverability damage?", answer: "Domain rehabilitation typically takes 2-8 weeks depending on severity. During that time, your campaigns are effectively grounded. Prevention through pre-send review in the AI Production Gate is orders of magnitude cheaper than recovery." },
    ],
    relatedTerms: ["can-spam-ai-messaging", "ai-brand-safety", "message-gating"],
    relatedPersonas: ["cold-email-infrastructure", "deliverability-agencies"],
    relatedComparison: "quality-gate-vs-deliverability-tooling",
  },

  "ai-brand-safety": {
    term: "AI Brand Safety",
    category: "Training Data",
    definition: "Protecting brand reputation by ensuring AI-generated communications maintain approved tone, messaging standards, and factual accuracy.",
    fullDescription: "AI brand safety is making sure your AI doesn't embarrass you. Language models don't understand your brand. They don't know your tone guidelines, your approved terminology, your positioning, or the claims you can and can't make. They generate plausible-sounding content that may or may not align with your standards. So your AI might write an email that's too aggressive, use a competitor's name, hallucinate a product feature, or crack a joke that doesn't land. Each message represents your brand — and at AI scale, you're sending thousands of brand impressions without a human ever seeing them. Unless you gate them. An AI Production Gate with brand rubrics evaluates every message against your approved tone, terminology, factual claims, and positioning before it ships.",
    whyItMatters: "One off-brand AI email becomes a screenshot on social media. One hallucinated product feature becomes a prospect's expectation you can't meet. One aggressive sales message becomes the reason a deal goes cold. AI-generated content represents your brand at massive scale — which means brand safety issues multiply at massive scale too. Systematic pre-send evaluation isn't perfectionism. It's brand protection.",
    howBookbagHelps: "Bookbag enforces your brand standards through configurable rubrics: approved terminology, tone guidelines, factual claims, positioning language, and off-limits topics. Every AI message is evaluated against these standards in the AI Production Gate. Off-brand content gets needs_fix for correction or blocked for authority escalation. Rubrics are version-controlled, so you can update brand standards as they evolve — and the immutable audit trail shows exactly which version applied to each decision.",
    metaTitle: "AI Brand Safety for Outbound | Bookbag Intelligence",
    metaDescription: "Protect your brand from AI-generated messaging that's off-tone, inaccurate, or embarrassing. Learn how brand safety rubrics prevent AI brand damage.",
    faqs: [
      { question: "What are common AI brand safety issues?", answer: "Wrong tone (too aggressive, too casual, too corporate), hallucinated product features, inaccurate pricing, competitor mentions, off-brand humor, claims that don't match your positioning, and content that could embarrass the brand if screenshotted. AI doesn't know your brand — it generates what sounds plausible." },
      { question: "How do you enforce brand standards with AI?", answer: "Define brand rubrics in your AI Production Gate: approved terminology, tone guidelines, factual claims, positioning language. Every AI message is evaluated against these standards. Off-brand content gets flagged and corrected before it reaches a prospect. The corrections also become training data that teaches the AI your actual brand voice." },
      { question: "Can brand standards evolve over time?", answer: "Absolutely. Rubrics are version-controlled through rubric versioning. Update your brand standards as they evolve. The immutable audit trail shows which version applied to each historical decision — so you always know what rules were in effect when a message was reviewed." },
    ],
    relatedTerms: ["ai-hallucination-detection", "ai-message-quality", "taxonomy-config"],
    relatedPersonas: ["sales-enablement", "b2b-saas"],
    relatedComparison: "bookbag-vs-prompt-engineering",
  },

  "sft-export": {
    term: "SFT Export",
    category: "Training Data",
    definition: "Supervised Fine-Tuning export — extracting human-corrected message pairs (original AI output + approved correction) in formats suitable for fine-tuning language models.",
    keyInsight: "Every human correction in the AI Production Gate is already a training example waiting to happen. SFT export just packages it for your model to learn from.",
    fullDescription: "SFT (Supervised Fine-Tuning) export takes the corrections your human reviewers make in the AI Production Gate and packages them as training data. The format is simple: each pair has the original AI-generated message (the input your model produced) and the human-approved correction (what it should have produced). Feed these pairs to your fine-tuning pipeline, and you're directly teaching the model what 'good' looks like — not in the abstract, but for your specific brand, your compliance requirements, your tone, your product facts. It's the most straightforward form of training data because there's no ambiguity: the human said 'this is wrong, here's what's right.' The model learns the difference.",
    whyItMatters: "Here's the training data flywheel: your AI generates messages. The AI Production Gate catches the ones that need fixing. Humans correct them. Those corrections become SFT pairs. You retrain the model. The AI produces better messages. Fewer corrections needed. Higher safe_to_deploy rates. Lower review costs. The flywheel is powered by the corrections your team is already making — SFT export just captures the value.",
    howBookbagHelps: "Bookbag exports approved-only SFT pairs from needs_fix and blocked corrections — the gold standard rewrites your reviewers create during normal operations. Exports are tenant-isolated (your data stays yours), include full provenance (which reviewer, which rubric, when), and require no additional AI evaluation — it's pure data transformation of human decisions. Standard JSON format, compatible with common fine-tuning frameworks.",
    bookbagFeatures: [
      { title: "Approved-only export", description: "Only corrections that were reviewed and approved become training data. No unapproved or draft corrections contaminate your training set." },
      { title: "Full provenance metadata", description: "Every training pair includes which reviewer made the correction, which rubric applied, and when — so you can trace any training example back to its source." },
      { title: "Standard format output", description: "JSON pairs with input/target structure, compatible with common fine-tuning frameworks. Plug directly into your training pipeline." },
    ],
    metaTitle: "SFT Export for AI Training Data | Bookbag Intelligence",
    metaDescription: "Export supervised fine-tuning data from human corrections. Turn AI message reviews into training pairs that improve your models over time.",
    faqs: [
      { question: "What format are SFT exports?", answer: "Standard JSON training pairs: input (the original AI message) and target (the human-approved correction). Compatible with common fine-tuning frameworks. No proprietary format, no lock-in." },
      { question: "Do SFT exports include all messages?", answer: "Only approved corrections from needs_fix and blocked items. safe_to_deploy messages aren't included because they don't represent corrections — there's nothing for the model to learn from a message that was already right. The training data is exclusively from human-reviewed, human-corrected, human-approved items." },
      { question: "How much training data do I need?", answer: "Less than you'd think. Even 50-100 high-quality correction pairs can measurably improve model performance for specific tasks like tone, compliance language, or product accuracy. Most teams accumulate that volume within the first month of running messages through the AI Production Gate." },
    ],
    relatedTerms: ["dpo-training-data", "preference-ranking-data", "gold-standard-rewrites"],
    relatedPersonas: ["vp-product", "ai-sdr-vendors"],
    relatedComparison: "rewrite-workflow-vs-prompt-tweaks",
  },

  "dpo-training-data": {
    term: "DPO Training Data",
    category: "Training Data",
    definition: "Direct Preference Optimization data — pairs of AI outputs where one version is human-preferred over another, used to align language models with human quality standards.",
    keyInsight: "SFT teaches your AI what to say. DPO teaches it what to prefer. That's a deeper, more durable form of alignment — and every needs_fix correction generates a DPO pair automatically.",
    fullDescription: "DPO (Direct Preference Optimization) is a training technique that teaches models to prefer the kinds of outputs humans approve. Instead of just showing the model 'here's a good example' (that's SFT), DPO shows it the pair: 'here's what you generated (rejected) and here's what the expert preferred (approved).' That comparison teaches the model the difference between its instincts and your standards. In the AI Production Gate, DPO data comes naturally from needs_fix corrections — the original AI message is the rejected version, and the human-corrected gold standard rewrite is the preferred version. The preference signal is real, not synthetic. It came from a qualified human reviewer applying your rubric in a production context. That's what makes production DPO data so valuable compared to synthetic preference datasets.",
    whyItMatters: "DPO is more fine-grained than SFT alone. SFT says 'produce this.' DPO says 'when you're choosing between outputs like these, prefer the one that looks like this.' It directly reshapes the model's generation tendencies toward your quality standards. And because every needs_fix correction in the AI Production Gate naturally produces a DPO pair, you're generating alignment data as a byproduct of quality review. The training data flywheel spins without extra effort.",
    howBookbagHelps: "Bookbag automatically structures every needs_fix and blocked correction as a DPO preference pair: the original AI message (rejected) paired with the approved gold standard rewrite (preferred). Every exported pair includes full provenance — which rubric applied, which reviewer made the correction, when it happened, and what verdict triggered the review. No synthetic data. No additional annotation tasks. Real preference signals from real production review.",
    bookbagFeatures: [
      { title: "Automatic pair structuring", description: "Every correction is automatically formatted as a DPO preference pair — original (rejected) vs. gold standard rewrite (preferred). No extra annotation work." },
      { title: "Production-grade provenance", description: "Each pair includes which rubric applied, which reviewer corrected, and when — traceable, real-world preference signals, not synthetic data." },
      { title: "Combined with SFT export", description: "Use DPO pairs alongside SFT data for comprehensive model training. Corrections (SFT) plus preferences (DPO) from the same review workflow." },
    ],
    metaTitle: "DPO Training Data from AI Reviews | Bookbag Intelligence",
    metaDescription: "Generate DPO preference pairs from human AI message reviews. Turn corrections into model alignment data that teaches AI what quality looks like.",
    faqs: [
      { question: "How is DPO different from SFT?", answer: "SFT teaches the model what to produce — 'given this input, generate this output.' DPO teaches the model what to prefer — 'between these two versions, humans prefer this one.' DPO is often more effective for aligning generation quality with human standards because it directly shapes the model's preferences, not just its outputs." },
      { question: "Where do DPO pairs come from?", answer: "Every needs_fix correction in the AI Production Gate creates a DPO pair automatically: the original AI message (rejected) and the human-corrected gold standard rewrite (preferred). The reviewer's correction is the preference signal. No extra annotation work needed." },
      { question: "How many DPO pairs do I need?", answer: "Research shows meaningful alignment improvements with as few as 100-500 high-quality preference pairs. Most teams generate that volume organically within the first few weeks of running messages through the AI Production Gate. Quality matters more than quantity — and production corrections are as high-quality as preference data gets." },
    ],
    relatedTerms: ["sft-export", "preference-ranking-data", "gold-standard-rewrites"],
    relatedPersonas: ["vp-product", "ai-sdr-vendors"],
    relatedComparison: "rewrite-workflow-vs-prompt-tweaks",
  },

  "preference-ranking-data": {
    term: "Preference Ranking Data",
    category: "Training Data",
    definition: "Ordered rankings of multiple AI output variations by human reviewers, used to train models on quality gradients rather than binary good/bad distinctions.",
    fullDescription: "Preference ranking goes beyond 'good vs. bad.' Instead of asking a reviewer to pick between two versions, you give them multiple AI output variations and ask: rank these from best to worst. The result is ordered quality gradients — data that captures nuance. A message might be 'technically correct but poorly toned,' and another might be 'great tone but factually off.' Ranking data captures these distinctions where binary labels flatten them. This is particularly valuable when you're comparing different model outputs, testing prompt variations, or evaluating generation strategies against each other. The rankings teach your model not just what's good, but what's better — degrees of quality that make AI outputs more consistently excellent rather than just acceptable.",
    whyItMatters: "Binary good/bad labels throw away information. Was the message bad because of tone or accuracy? Was it good but could be better? Ranking data preserves these gradients. It helps models understand that quality isn't a switch — it's a spectrum. For teams that are past the basics and optimizing for excellence rather than just avoiding failures, ranking data is the tool that gets you there.",
    howBookbagHelps: "Bookbag supports ranking tasks where reviewers order multiple AI variations from best to worst. Rankings are exportable as training data with full provenance: which reviewer ranked, which rubric applied, and the ordered results. Combined with SFT and DPO data from the AI Production Gate, ranking data creates a comprehensive training data set that covers corrections (SFT), preferences (DPO), and quality gradients (ranking).",
    metaTitle: "Preference Ranking Data for AI Training | Bookbag",
    metaDescription: "Generate preference ranking data from human reviews of AI outputs. Capture quality gradients that improve model alignment beyond binary good/bad labels.",
    faqs: [
      { question: "How is ranking different from DPO?", answer: "DPO uses pairs: preferred vs. rejected. One wins, one loses. Ranking orders multiple outputs from best to worst — capturing finer quality gradients. If DPO is a head-to-head matchup, ranking is a full leaderboard. Use both for different purposes." },
      { question: "When should I use ranking vs. DPO?", answer: "Use DPO when you have clear before/after corrections from the AI Production Gate — the original message vs. the gold standard rewrite. Use ranking when comparing multiple model outputs, testing prompt variations, or evaluating different generation strategies against each other. Different tools for different training objectives." },
      { question: "How many items should reviewers rank?", answer: "Typically 2-5 variations per ranking task. More than 5 gets cognitively difficult for reviewers and produces less reliable rankings. The sweet spot is usually 3-4 variations, which balances information richness with reviewer accuracy." },
    ],
    relatedTerms: ["dpo-training-data", "sft-export", "annotator-calibration"],
    relatedPersonas: ["vp-product", "ai-sdr-vendors"],
    relatedComparison: "rewrite-workflow-vs-prompt-tweaks",
  },

  "gold-standard-rewrites": {
    term: "Gold Standard Rewrites",
    category: "Training Data",
    definition: "Expert-corrected versions of AI-generated messages that serve as the approved reference examples for quality, tone, compliance, and effectiveness.",
    keyInsight: "The correction IS the training data. Every rewrite teaches your AI what 'good' looks like in your specific context.",
    fullDescription: "A gold standard rewrite is what happens when a human reviewer takes a flawed AI message and fixes it — that corrected version is now the gold standard. It's the authoritative example of what the AI should have produced. Here's why that matters: the gold standard rewrite does three jobs simultaneously. First, it replaces the flawed original and ships to the recipient — fixing the immediate problem. Second, it becomes training data (SFT and DPO pairs) that teaches your AI what 'right' looks like for your specific brand, compliance requirements, and tone. Third, it builds an approved messaging library — a growing collection of 'this is how we talk' examples that your team can reference. Every needs_fix correction and every blocked item correction produces a gold standard rewrite. They're created as a natural byproduct of the review process, not as a separate task.",
    whyItMatters: "Gold standard rewrites are the highest-value output of the AI Production Gate. They solve today's problem (fixing a bad message before it ships), solve tomorrow's problem (training data that makes the AI better), and build long-term institutional knowledge (approved messaging library). A growing library of gold rewrites is a compounding asset — every rewrite makes your AI smarter and your review process cheaper. That's the training data flywheel in action.",
    howBookbagHelps: "Every needs_fix and blocked correction in Bookbag produces a gold standard rewrite automatically. The rewrites are cataloged, searchable, and exportable as SFT and DPO training data. The gold rewrite library grows organically as your team reviews more messages — no separate annotation task, no extra work. Each rewrite carries full provenance in the immutable audit trail: who wrote it, which rubric it was correcting against, and when.",
    bookbagFeatures: [
      { title: "Automatic gold capture", description: "Every QA and SME correction automatically becomes a gold standard rewrite. No separate workflow. The review IS the gold standard creation." },
      { title: "Triple-duty output", description: "Each gold rewrite simultaneously fixes the message for delivery, creates training data for model improvement, and builds the approved messaging library." },
      { title: "Searchable rewrite library", description: "Gold rewrites are cataloged and searchable — your team can reference approved examples when creating new content or calibrating reviewers." },
    ],
    metaTitle: "Gold Standard Rewrites in AI Quality | Bookbag Intelligence",
    metaDescription: "Gold standard rewrites are expert-corrected AI messages that set quality benchmarks. Learn how they improve AI models and build approved messaging libraries.",
    faqs: [
      { question: "Who creates gold standard rewrites?", answer: "QA reviewers and SMEs create them as part of normal review operations. When a reviewer corrects a needs_fix message or an SME fixes a blocked message, the corrected version is the gold standard rewrite. It's not a separate task — it's the natural output of the review process." },
      { question: "How are gold rewrites used?", answer: "Three ways, all simultaneously: (1) the corrected message ships to the recipient, replacing the flawed original, (2) the correction becomes SFT and DPO training data you can export to retrain your models, and (3) it joins the searchable approved messaging library your team can reference." },
      { question: "How many gold rewrites should we aim for?", answer: "Quality over quantity, always. Even 20 gold standard rewrites from a free audit are valuable for initial model improvement. In production, most teams accumulate hundreds within the first month from needs_fix corrections alone. The library grows organically as a byproduct of the review workflow." },
    ],
    relatedTerms: ["sft-export", "dpo-training-data", "needs-fix"],
    relatedPersonas: ["sales-enablement", "ai-sdr-vendors"],
    relatedComparison: "rewrite-workflow-vs-prompt-tweaks",
  },

  "human-in-the-loop-ai": {
    term: "Human-in-the-Loop AI",
    category: "Workflows",
    definition: "An AI system design where human reviewers participate in the decision-making process, providing oversight, corrections, and authority that the AI alone cannot provide.",
    keyInsight: "Human-in-the-loop isn't 'humans checking AI's homework.' It's a system where human authority and AI scale reinforce each other — and every human decision makes the AI smarter.",
    fullDescription: "Human-in-the-loop (HITL) means humans are wired into the AI's operational workflow as an essential component, not an afterthought. In AI outbound, this means human reviewers evaluate AI-generated messages, render verdicts (safe_to_deploy / needs_fix / blocked), provide corrections through gold standard rewrites, and exercise human authority over high-risk items through authority escalation. But here's the distinction that matters: HITL isn't just 'having humans review stuff.' It's a designed system. Defined authority levels — annotators, QA reviewers, SMEs — each with clear scope. Rubric-driven evaluation, not subjective opinions. Calibration processes to keep reviewers consistent. And critically, every human decision feeds back into the system as training data, calibration signals, and immutable audit trail records. The humans don't just catch problems. They generate the data that makes the AI better.",
    whyItMatters: "Pure automation scales but lacks judgment — your AI will confidently send hallucinated claims at volume. Pure human review has judgment but doesn't scale — you can't hire enough people to read every message. Human-in-the-loop combines the scale of AI with the judgment of humans. The AI handles the easy stuff (safe_to_deploy). Humans focus on the hard stuff (needs_fix and blocked). And every human decision feeds back as training data that makes the AI handle more of the easy stuff over time. That's the training data flywheel.",
    howBookbagHelps: "Bookbag is built from the ground up as a human-in-the-loop system. Humans operate at three authority levels — annotators for routine review, QA reviewers for corrections, SMEs for authority escalation on blocked items. Every human decision is captured in the immutable audit trail, produces exportable training data (SFT, DPO), and contributes to calibration metrics. The system gets smarter as humans work, and humans focus on fewer items as the system gets smarter.",
    bookbagFeatures: [
      { title: "Three-tier human authority", description: "Annotators, QA reviewers, and SMEs — each with defined scope and authority. The right human expertise meets the right problem." },
      { title: "Decision-to-data pipeline", description: "Every human verdict, correction, and rationale is automatically captured as training data, audit records, and calibration signals." },
      { title: "Flywheel architecture", description: "Human corrections become training data that improves the AI, which increases safe_to_deploy rates, which reduces the human review burden. The system improves itself." },
    ],
    metaTitle: "Human-in-the-Loop AI for Outbound | Bookbag Intelligence",
    metaDescription: "Human-in-the-loop AI combines AI scale with human judgment. Learn how HITL workflows improve AI outbound quality and produce training data.",
    faqs: [
      { question: "Doesn't human-in-the-loop slow things down?", answer: "Only for the messages that need it — and those are exactly the ones you want a human looking at. safe_to_deploy messages are cleared for delivery instantly with zero human delay. Humans focus on the needs_fix and blocked items. The result is actually faster than reviewing everything manually, because the majority of messages skip the human queue entirely." },
      { question: "How is HITL different from just having humans review?", answer: "Structure. 'Human review' can be ad-hoc — different people applying different standards with no data capture. HITL is a designed system: defined authority levels, rubric-driven evaluation, calibration processes, immutable audit trail, and training data export. Every human decision produces compounding value. Ad-hoc review produces opinions." },
      { question: "Does HITL replace AI?", answer: "The opposite. HITL makes AI better. Human corrections become SFT and DPO training data. Retrain the model and safe_to_deploy rates climb. Over time, the AI produces more correct output, reducing the human review burden. Humans don't replace AI. They teach it. That's the training data flywheel." },
    ],
    relatedTerms: ["ai-production-gate", "sme-escalation", "annotator-calibration"],
    relatedPersonas: ["vp-product", "compliance-officer"],
    relatedComparison: "human-review-vs-automated-qa",
  },

  "qa-review-workflow": {
    term: "QA Review Workflow",
    category: "Workflows",
    definition: "A structured process where quality assurance reviewers evaluate, correct, and approve AI-generated content using defined rubrics and authority levels.",
    keyInsight: "The QA workflow isn't a bottleneck — it's a factory. Every review produces a fixed message, a training data point, and an audit record. Three outputs from one action.",
    fullDescription: "The QA review workflow is the engine that handles needs_fix items in the AI Production Gate. Here's how it works: a message fails the rubric check and gets flagged as needs_fix. It enters the QA queue with full context — the specific rubric citations, the flagged issues, the original AI output. A QA reviewer evaluates it, makes corrections (gold standard rewrites: tone fixes, fact corrections, compliance language), and approves the fixed version. If the issue exceeds their authority, they escalate to an SME through authority escalation. The whole workflow is rubric-driven, not opinion-driven. Every reviewer applies the same standards because they're evaluating against the same rubric. That consistency is what makes the corrections valuable as training data — they represent your standards, not one reviewer's preferences.",
    whyItMatters: "Without a structured QA workflow, review is a mess. Different people apply different standards. Corrections happen in email threads and chat messages. Patterns in AI failures go completely unnoticed. Nobody can prove who reviewed what, or when. A structured workflow solves all of this: consistent rubric-driven evaluation, captured corrections that become training data, authority escalation paths for hard calls, and an immutable audit trail documenting every decision.",
    howBookbagHelps: "Bookbag provides the complete QA workflow infrastructure: queue management that routes needs_fix items to available reviewers, a rubric-driven evaluation interface with flagged issues and citations, correction tools for creating gold standard rewrites, one-click authority escalation to SMEs for items that exceed QA scope, and automatic training data capture from every review decision. Every correction feeds the training data flywheel. Every decision feeds the immutable audit trail.",
    bookbagFeatures: [
      { title: "Contextual review interface", description: "QA reviewers see the flagged issues, rubric citations, and original message together. No context-switching, no guessing about what needs attention." },
      { title: "One-click authority escalation", description: "When an issue exceeds QA scope, escalate to an SME with one click. Full context transfers automatically. No lost information." },
      { title: "Automatic triple output", description: "Every QA review simultaneously produces a fixed message for delivery, training data for model improvement, and an audit record for compliance." },
    ],
    metaTitle: "QA Review Workflow for AI Content | Bookbag Intelligence",
    metaDescription: "A QA review workflow structures how reviewers evaluate and correct AI content. Learn how structured review improves quality and generates training data.",
    faqs: [
      { question: "What does a QA reviewer actually do?", answer: "They receive flagged AI messages in their queue with the specific rubric violations highlighted. They evaluate the message, make corrections (gold standard rewrites — tone fixes, fact corrections, compliance language adjustments), and approve the corrected version. If the issue exceeds their authority — say, a potential compliance violation — they escalate to an SME with one click." },
      { question: "How are QA reviewers calibrated?", answer: "Through gold set evaluation (reviewing pre-labeled examples with known correct answers), rubric training sessions, and ongoing quality sampling. Inter-annotator agreement metrics track consistency across reviewers. If consistency drifts, the data shows it and triggers recalibration." },
      { question: "How many QA reviewers do I need?", answer: "Depends on your message volume and turnaround SLA. Bookbag manages workforce scaling. Most teams start with 2-5 reviewers for pilot volume and scale based on needs_fix rates and turnaround requirements. As safe_to_deploy rates climb through the training data flywheel, QA volume decreases." },
    ],
    relatedTerms: ["needs-fix", "annotator-calibration", "taxonomy-config"],
    relatedPersonas: ["revops", "head-of-cs"],
    relatedComparison: "bookbag-vs-internal-qa",
  },

  "taxonomy-config": {
    term: "Taxonomy Config",
    category: "Workflows",
    definition: "A configurable schema that defines the rubrics, label definitions, scoring criteria, and review rules for a specific project or compliance domain.",
    keyInsight: "Taxonomy config is how you turn 'our brand standards' and 'our compliance rules' into machine-enforceable rubrics — without writing code.",
    fullDescription: "Taxonomy configuration is the layer that turns your requirements into enforceable rules for the AI Production Gate. Instead of building custom code for each compliance domain or brand standard, you express your rules as configuration: label definitions (what safe_to_deploy / needs_fix / blocked mean for your context), scoring criteria (what triggers each verdict), failure categories (types of issues your rubric catches), escalation thresholds (when to route to SME through authority escalation), and required evidence fields. The taxonomy is version-controlled through rubric versioning, so every review decision can be traced to the specific rules that applied at the time. New project? Reconfigure the taxonomy. Different compliance domain? New taxonomy. No custom engineering required — which means pilots launch in days, not weeks.",
    whyItMatters: "Every organization has different rules. A financial services firm following FINRA 2210 has completely different requirements than a B2B SaaS company worried about product accuracy. Without configurable taxonomies, you'd need to rebuild the review system for each customer. With them, you reconfigure. And because taxonomies are version-controlled, the immutable audit trail always shows exactly which rules applied to each historical decision — even as rules evolve.",
    howBookbagHelps: "Bookbag's taxonomy system lets you define rubrics, label sets, scoring criteria, and escalation rules as configuration — no code required. The annotation interface dynamically adapts to the active taxonomy, showing reviewers the right fields, criteria, and escalation options for their specific project. Every review decision references the taxonomy version used. Template taxonomies for common use cases (FINRA compliance, B2B SaaS outbound, cold email quality) give you a starting point you can customize.",
    metaTitle: "Taxonomy Config for AI Review | Bookbag Intelligence",
    metaDescription: "Taxonomy config defines the rubrics and rules for AI content review. Learn how configurable taxonomies make review adaptable and auditable.",
    faqs: [
      { question: "What goes into a taxonomy config?", answer: "The core components: label definitions (what safe_to_deploy / needs_fix / blocked mean in your context), scoring criteria (what specific issues trigger each verdict), required fields (evidence, rationale — what reviewers must document), escalation thresholds (which issues trigger authority escalation to SMEs), and domain-specific rules unique to your compliance requirements." },
      { question: "Can we change the taxonomy mid-project?", answer: "Absolutely. Taxonomy changes create a new version through rubric versioning. All past reviews keep their reference to the old version. All new reviews use the updated version. The immutable audit trail shows exactly which rules applied to which decisions. Nothing is overwritten." },
      { question: "Do you provide template taxonomies?", answer: "Yes. Bookbag provides taxonomy templates for common use cases — FINRA compliance, B2B SaaS outbound, cold email quality, healthcare messaging, and more. These give you a proven starting point that you customize to your specific requirements. Most teams launch pilots from templates in days." },
    ],
    relatedTerms: ["rubric-versioning", "ai-audit-trail", "qa-review-workflow"],
    relatedPersonas: ["compliance-officer", "revops"],
    relatedComparison: "bookbag-vs-internal-qa",
  },

  "rubric-versioning": {
    term: "Rubric Versioning",
    category: "Workflows",
    definition: "The practice of version-stamping review rubrics so every verdict can be traced to the specific rules that applied at the time of the decision.",
    fullDescription: "Rubric versioning means every revision of your review rules gets a unique version identifier. When a reviewer renders a verdict on a message, the rubric version is recorded alongside the verdict in the immutable audit trail. Six months later, you can look at any decision and see exactly which rules applied — not the current rules, not the rules you think were active, but the documented version that was in effect at that specific moment. This also enables safe rubric evolution: update your compliance rules, refine your tone guidelines, tighten your accuracy standards — and every historical decision still references the version it was evaluated against. Nothing gets retroactively invalidated. New rules apply going forward. Old decisions keep their context.",
    whyItMatters: "Compliance rubrics change. Regulations evolve. Products update. Best practices shift. Without versioning, you're stuck: either you can't prove which rules applied to a past review (bad for audits), or you're afraid to update rules because it might invalidate previous decisions (bad for improvement). Rubric versioning solves both problems. Regulators expect this level of traceability for supervised AI communications — and procurement teams evaluate it during vendor assessment.",
    howBookbagHelps: "Bookbag automatically version-stamps every taxonomy change. Every review decision in the immutable audit trail includes the rubric version that was active. You can compare decisions across versions to see exactly how rule changes impacted verdicts — did tightening the compliance rubric increase blocked rates? Did refining the tone criteria change needs_fix patterns? The data tells you.",
    metaTitle: "Rubric Versioning for AI Review | Bookbag Intelligence",
    metaDescription: "Rubric versioning stamps every review decision with the rules that applied. Learn why version-controlled rubrics matter for compliance and audit.",
    faqs: [
      { question: "Why is rubric versioning important for compliance?", answer: "When a regulator asks 'what standard was this message reviewed against?' — you need a precise answer. Not 'our current rubric' but 'rubric version 3.2, which was active on that date, with these specific criteria.' Version stamps provide this traceability. Without them, your compliance documentation has a gap that examiners will find." },
      { question: "What happens when we update a rubric?", answer: "A new version is created automatically. All future reviews use the new version. All past reviews retain their reference to the old version in the immutable audit trail. Nothing is overwritten. Nothing is lost. You evolve the rules without invalidating the history." },
      { question: "Can we roll back to a previous rubric version?", answer: "Yes. Activating a previous version creates a new version identifier that references the old rules. The complete version history remains intact for audit purposes. You're not 'reverting' — you're creating a new version that happens to match a previous one." },
    ],
    relatedTerms: ["taxonomy-config", "ai-audit-trail", "audit-ready-review"],
    relatedPersonas: ["compliance-officer", "finserv"],
    relatedComparison: "bookbag-vs-manual-review",
  },

  "annotator-calibration": {
    term: "Annotator Calibration",
    category: "Workflows",
    definition: "The process of training and aligning human reviewers to apply rubrics consistently, measured through gold set evaluation and inter-annotator agreement metrics.",
    keyInsight: "If two reviewers would give the same message different verdicts, your AI Production Gate is unreliable. Calibration is what makes every verdict trustworthy regardless of who reviewed it.",
    fullDescription: "Annotator calibration is how you ensure that Reviewer A and Reviewer B apply the same standards when evaluating the same AI-generated message. Without calibration, your verdicts are basically random — one reviewer says safe_to_deploy, another says needs_fix, and your AI Production Gate becomes unreliable. Calibration works through several mechanisms: gold set testing (reviewers evaluate pre-labeled examples with known correct answers to verify they apply rubrics correctly), rubric training sessions, ongoing quality sampling (randomly re-reviewing production items to check consistency), and inter-annotator agreement metrics (measuring how often different reviewers agree on the same items). Calibration isn't a one-time event. Standards evolve, new failure patterns emerge, and reviewer consistency naturally drifts. Ongoing calibration catches that drift before it undermines your gate.",
    whyItMatters: "Inconsistent review is worse than no review because it creates false confidence. You think your AI Production Gate is catching problems, but the verdicts depend on which reviewer happened to get the message. That's not quality control — it's a coin flip. Calibration ensures every verdict is trustworthy regardless of reviewer. It's also what makes your training data reliable: if corrections are inconsistent, the training data teaches your AI conflicting standards.",
    howBookbagHelps: "Bookbag supports the full calibration workflow: gold set management (curated pre-labeled examples for reviewer testing), quality sampling (automatic random re-review of production items), and inter-annotator agreement tracking (metrics showing reviewer consistency). New reviewers calibrate against gold sets before touching production items. Ongoing sampling catches consistency drift. When agreement metrics drop, the data shows it and triggers recalibration before it affects verdict quality.",
    bookbagFeatures: [
      { title: "Gold set management", description: "Curate and manage pre-labeled examples with known correct answers. New reviewers prove they can apply your rubric correctly before handling production items." },
      { title: "Automatic quality sampling", description: "Random re-review of production items catches consistency drift. You see the data before it becomes a problem." },
      { title: "Agreement tracking dashboard", description: "Inter-annotator agreement metrics show reviewer consistency across the team. When consistency drops, the data triggers recalibration." },
    ],
    metaTitle: "Annotator Calibration for AI Review | Bookbag Intelligence",
    metaDescription: "Annotator calibration aligns human reviewers to apply consistent standards. Learn how gold sets, sampling, and agreement metrics ensure reliable AI review.",
    faqs: [
      { question: "What is a gold set?", answer: "A curated collection of pre-labeled examples with known correct answers, reviewed and approved by SMEs. New reviewers evaluate gold set items to verify they apply rubrics correctly before handling production work. Think of it as the final exam before a reviewer goes live." },
      { question: "How is calibration measured?", answer: "Three metrics: gold set agreement (does the reviewer match the known correct answers?), inter-annotator agreement (do different reviewers give the same verdict on the same items?), and QA sampling rates (how often do quality checks catch errors?). Together, these tell you whether your verdicts are reliable." },
      { question: "How often should reviewers be recalibrated?", answer: "Initial calibration before they touch production items. Ongoing monitoring through quality sampling. Formal recalibration sessions when rubrics change, when agreement metrics drop, or when new failure patterns emerge. Calibration isn't a one-time event — it's continuous quality assurance on the quality assurance." },
    ],
    relatedTerms: ["qa-review-workflow", "taxonomy-config", "gold-standard-rewrites"],
    relatedPersonas: ["revops", "head-of-cs"],
    relatedComparison: "human-review-vs-automated-qa",
  },

  "audit-ready-review": {
    term: "Audit-Ready Review",
    category: "Compliance",
    definition: "A review process designed to produce documentation that satisfies regulatory examination, enterprise procurement, and compliance audit requirements from the start.",
    keyInsight: "If you build audit readiness into the review workflow, you never have to scramble to 'prepare for an audit' — the documentation already exists as a byproduct of doing the work.",
    fullDescription: "Audit-ready review means the documentation is built into the workflow, not bolted on after. Every review decision in the AI Production Gate automatically produces a record that would satisfy a regulatory examiner, enterprise procurement team, or compliance auditor — without anyone doing extra documentation work. The record includes: who reviewed it (attributable human authority), when (timestamped), what standard applied (rubric version through rubric versioning), what verdict was given (safe_to_deploy / needs_fix / blocked), why (rationale), supporting evidence (citations), and the record is immutable (can't be altered after the fact). When someone asks 'show me your AI supervision process' — you don't describe a process. You produce the records. That's audit-ready review.",
    whyItMatters: "Retroactively documenting review decisions for audits is expensive, error-prone, and sometimes impossible. 'We reviewed it, we just didn't document it' doesn't fly with regulators, procurement teams, or legal counsel. Building audit readiness into the review process means you're always prepared for examination — not scrambling when the examiner calls. For regulated industries, this is often the difference between passing and failing a regulatory review.",
    howBookbagHelps: "Every decision in the Bookbag AI Production Gate automatically captures the full audit record: reviewer identity, timestamp, rubric version, verdict, rationale, evidence citations, and corrections. Records are immutable (can't be altered) and tenant-isolated (your data stays yours). The immutable audit trail is a natural byproduct of the review workflow — no extra documentation work, no separate compliance task. When the auditor asks, you export the records.",
    bookbagFeatures: [
      { title: "Zero extra documentation work", description: "Audit records are generated automatically during normal review operations. Reviewers do their job; the audit trail builds itself." },
      { title: "Complete evidence chain", description: "Every record includes who reviewed, what standard applied, what verdict was given, what rationale was provided, and what evidence supports it. The full provenance chain." },
      { title: "Always-ready exports", description: "Immutable, tenant-isolated audit records are exportable anytime. When the auditor calls, you don't prepare — you produce." },
    ],
    metaTitle: "Audit-Ready Review for AI Content | Bookbag Intelligence",
    metaDescription: "Audit-ready review builds compliance documentation into the AI review process. Every decision is attributable, timestamped, and evidence-backed.",
    faqs: [
      { question: "What makes a review 'audit-ready'?", answer: "Six elements: attributable (who reviewed — human authority is documented), timestamped (when the decision was made), rubric-referenced (which rules applied — rubric versioning), rational (why the verdict was given), evidence-backed (supporting citations), and immutable (the record can't be altered). All six are captured automatically during normal review operations in the AI Production Gate." },
      { question: "Who typically audits AI review processes?", answer: "Regulatory examiners (FINRA, state regulators), enterprise procurement teams during vendor assessment, internal compliance officers during periodic reviews, and external auditors during annual examinations. Each audience has different priorities, but the core elements — attribution, timestamps, standards, rationale, evidence, immutability — satisfy all of them." },
      { question: "Do we need to do extra work for audit readiness?", answer: "No. That's the whole point. Bookbag captures audit-ready records automatically during normal review operations. The immutable audit trail is a byproduct of the review workflow, not a separate task. Your reviewers don't do 'documentation work' — they do reviews, and the documentation happens." },
    ],
    relatedTerms: ["ai-audit-trail", "rubric-versioning", "sme-escalation"],
    relatedPersonas: ["compliance-officer", "finserv"],
    relatedComparison: "bookbag-vs-manual-review",
  },
}

export function getGlossaryTerm(slug) {
  return glossaryTerms[slug] || null
}

export function getAllGlossarySlugs() {
  return Object.keys(glossaryTerms)
}
